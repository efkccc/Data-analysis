---
title: Chudi Thesis
author: Chudi Gong
date: 02/09/2020
output: pdf_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2) # for plotting graphs
library(citr) # for citation/reference management
library(tinytex) # for LaTex file compiling 
library(dplyr) # data manipulation
library(magrittr) # readable code (for operators etc.)
library(readr) # for reading data files (csv etc.)
library(tidyr) # for tidying data
library(data.table) # for data manipulation 
library(afex)
library(RColorBrewer) # provides color palletes for graph plotting 
library(papaja) # apa stype for reporting data
library(MOTE) # required package for use of 'papaja'
library(MESS) # statistical functions used in conjunstion with papaja
library(lmerTest) # for mixed effects model fit
library(broom.mixed) # to tidy model fit coefficients
getwd()
se <- function(x) sqrt(var(x)/length(x)) 
```
# 1.Abstract

# 2.Introduction 
As human beings we frequently make statements such as ‘I am confident that…’, reflecting the importance of confidence in daily life. Confidence can be defined as ‘a belief about the validity of our own thought, knowledge or performance that relies on a subjective feeling.’ (Grimaldi, Lau, and Basso 2015), which is a type pf metacognitive judgement that has wide applications in various fields including medical diagnosis, financial choices and eyewitness testimony etc. The level of confidence towards one decision can effectuate other subsequent decisions either for oneself or others, such as when an investor decides to drop a stock investment after purchasing. Given its importance, the mechanism supporting confidence formation has been investigated extensively in both animals and humans, leading to developments of behavioural, neural and computational models. 

Confidence is a type of subjective judgement that are often used in conjunction with other objective behaviours and decisions, which range from the most fundamental perceptual process to value-based decisions. In this project the attention is drawn to confidence formation process specific for perception. Research investigating how confidence is formed with regards to perception commonly used detection and discrimination tasks, which are considered to represent the very basics of our perceptual abilities. A detection task requires participants to identify the presence of a stimulus (e.g. whether a dot is present or absent), whereas a discrimination task (sometimes labeled as categorization) involves distinguishing a target stimulus from other distractors based on certain features (e.g. whether the line is tilted to the left or the right). Although they are both perceptual tasks, evidence has suggested they can lead to differences in objective performance (e.g., Mack and Palmeri 2010), which has been investigated extensively in past decades. Until recent years, with the increasing popularity in investigating confidence formation, studies showed that detection and discrimination can also lead to distinctive subjective judgments of performance behaviourally (Meuwese et al. 2014) and neuroimaging data suggested different neural representations associated with judging confidence in these two tasks (e.g., Mazor, Friston, and Fleming 2020). Models that have been put forward to explain such differences can be broadly divided into two camps: 1.) Confidence judgments is a higher-order cognitive process that treats detection and discrimination distinctively due to the differences in the control of one’s internal state, such as attention; 2.) Behavioural and neural activities differ during confidence formation process as a response to the physical features of detection and discrimination at the perceptual level such as amount of sensory evidence. The former treats confidence judgements as a product of a distinctive cognitive process from basic sensory stimuli processing whereas the later considers confidence judgements arises through same inferential process as perceptual processing. 

The current project aims to explain the distinctive processes involved in confidence formation in detection and discrimination tasks from two alternative perspectives, Signal Detection Theory (SDT) and counter-factual reasoning, using a new paradigm which includes a novel discrimination task with features of detection task. The current experimental design will allow us to untangle the qualitative and quantitative differences in detection and discrimination that can potentially account for some of the previous findings. The following section will first review behavioural and neuroimaging evidence showing differences in detection and discrimination tasks, then discuss some models that have been put forward to explain such differences and finally propose the hypothesis that distinctive patterns arise in forming confidence in the two tasks as a result of their different distributions of signal (target) and noise (distractor) based on SDT.

## 2.1. Detection vs. Discrimination  
### 2.2.1. Dependence/independence between detection and discrimination
How do we detect and/or discriminate something? Does one first detect something is there (e.g. ‘I saw something is there’) and then further categorize it to certain type (e.g. ‘The thing I saw is a car.’), or do we know what it is immediately when we see something is there (e.g. ‘I saw a car there)? This is an important consideration for a wide range of research themes, as it affects the flexibility for researchers to use these two tasks in studies of other topics such as confidence formation. Detection are discrimination signal from noise, Discrimination are detecting the target features?
Mack and Palmeri (2010) used a paradigm to assess the level of dependence of detection and discrimination at the basic-level (car vs. boats) and superordinate level (e.g. car vs. people). They found that detection and basic-level object categorisation is supported by single mechanism as their results revealed a dependence between the success for the detection task and the success for basic-level categorisation task. However, the success for one to correctly categorize a target at superordinate level is not dependent on the success at detection, which suggests distinctive mechanisms supporting detection and superordinate categorisation, thus again highlighting the importance of differentiating the two for future research. 

These findings reflect the importance of distinguishing between different levels of discrimination (i.e. basic, ordinate and super-ordinate), challenging the view of one underlying perceptual ability for, or at least strong dependence between detection and discrimination suggested previously by Grill-Spector and Kanwisher (2005).

### 2.2.1. Absence in detection
Another important differences between detection and discrimination tasks is that in discrimination tasks sensory evidence is available for both alternative choices whereas in detection, virtually no sensory evidence is available for participants to make an absence decision. This makes the “absence” condition in detection tasks a particularly interesting case--how do we detect nothing, based on nothing? The concept of nothing sparked strong interests from philosophers and scientists for centuries. Beyond the linguistic meaning of nothing, philosopher often discuss this concept in the context of ‘not-being’(XX), while mathematicians consider nothing expressed as ‘empty-set’ or ‘zero’, but the consensus being that it implies some form of ‘non-existence’. The question is then how our brain encodes something that is not there and translate it into sensory experience which is later used for decisions. The absence of a stimulus is traditionally thought to be coded by the brain as baseline activity in contrast to more robust firing of neurons in the presence of a stimulus. Contrariwise, when stimulus with strength on a continuing spectrum was shown, only those evoking activities beyond the threshold were considered to be perceived as present (Victor De Lafuente and Ranulfo Romo 2005). However, despite the lack of sensory evidence in absence conditions, recent evidence has suggested that neurons encode both stimulus absence and presence when dissociated from motor response (Katharina Merten and Andreas Nieder 2012). In this study, they found that prefrontal cortex (PFC) is recruited in rhesus monkeys during stimulus present trials as well as absence trials using single neuron recordings. 

A similar line of research has investigated the representation of ‘empty set’, which is the precursor for zero, in the brain. As trivial as this concept might appear, it actually makes up a fundamental part of the mathematical continuum that has not been investigated until recent decades (Rinaldi and Girelli 2016). How does the brain react to an empty set, i.e. nothing in contrast to other numbers? It has been shown that a group of neurons in the posterior parietal cortex of monkeys were activated in response to numerosity ‘zero’ in a numerical operation task (Sumito Okuyama, Toshinobu Kuki, and Hajime Mushiake 2015). Furthermore, Okuyama (2015) and colleagues found two types of ‘zero’ neurons in the ventral intraparietal (VIP) area; an exclusive group that showed selective response to zero target in contrast to non-zero targets and a continuous group that encodes both zero and non-zero numerosities (from 0 to 4) with strength on a continuous spectrum. Their results showed the presence of active encoding of numerosity zero, i.e. empty set, both when treating it as a continuous and categorical variable. Similar results were found by (Ramirez-Cardenas, Moskaleva, and Nieder 2016). By training monkeys to perform a number matching task, they found that not only all recorded neurons in VIP area and prefrontal cortex responded to empty sets, some of them also discharged maximally to empty sets in comparison to other numerosities, i.e. zero selective, which constitutes an even higher proportion of VIP cells in comparison to findings from Sumito Okuyama, Toshinobu Kuki, and Hajime Mushiake (2015).

Together, these evidence suggest that the lack of sensory inputs in “absence” or “zero” does not necessarily imply inactivity of neurons in the brain. These findings challenge the belief that neural activity is proportional to the strength of sensory inputs, while it remains unclear where those representations of ‘nothing’ arises from and what consequences they lead to. One way to interpret the neural activities in response to ‘empty set’, for instance, is that they held explicit information of ‘nothing’ as working memory which is later used for the task as suggested by Sumito Okuyama, Toshinobu Kuki, and Hajime Mushiake (2015). Others have proposed that the lack of sensory signals leads brain to generate a quantitative representation of zero abstractly that is later positioned on a numerical continuum (Ramirez-Cardenas, Moskaleva, and Nieder 2016). Nonetheless, the uniqueness and level of cognitive demands required for representing nothing is unequivocal, which again potentially distinguishes detection tasks from discrimination tasks. This induces curiosity in understanding how subjective judgments are formed in these two tasks respectively.

## 2.2. Forming confidence in detection and discrimination 
### 2.2. Metacognition in detection and discrimination
The first level differences between detection and discrimination performance sparks interests in studying the differences in second level subjective experiences in these two perceptual tasks such as confidence. In psychophysics, the level of confidence observers rated towards their objective performances are studied in relation to objective performance based on the assumption that participants would give high confidence ratings when they believed their choice was correct and low confidence ratings if they are less certain. The correspondence between objective performance and subjective report reflects one’s ability to have insight into the objective correctness of their response, which is also known as one’s metacognitive sensitivity/accuracy. In contrast to rather substantial differences in objective performance across tasks, one’s metacognitive ability was found to remain relatively stable across different tasks which led to the notion of a general mechanism supporting metacognitive judgements (Song et al. 2011). However, subjective judgements do not necessarily reflect our objective behaviour and according to studies, can vary significantly between different types of task (Kanai, Walsh, and Tseng 2010).

The study by Kanai, Walsh, and Tseng (2010) was one of the first to show that failure of visual perception is not always picked up by one’s subjective awareness. They used a detection task in which participants were asked to report the presence of the target stimuli. The stimuli were manipulated in six different ways to create six different conditions: contrast, backward masking, flash suppression, dual task, attentional blink and spatial uncertainty, which can be further categorized into attentional difficulty (dual task, attentional blink and spatial uncertainty) and sensory difficulty (contrast, backward masking and flash suppression). Participants then reported their confidence ratings of the correctness of their response after each trial. Combining these ratings with objective performance, a pseudo type two receiver operating characteristics curve (Type-II ROC) was computed. Metacognitive sensitivity is quantified by the area under these Type-II ROC curves, known as AUC. Beyond using the traditional Type II AUC measurements as shown in Fig.1A, they developed a new measure termed Subjective Discriminability of Invisibility (SDI) to measure how accurate participants can adjust their ratings accordingly to their task performance. SDI was developed based on computing the Type II performance but only for misses and correct rejections, i.e. trials in which participants reports absence of the stimulus (Fig.1B). To compare the metacognitive sensitivity in six different conditions, the objective performance of participants was controlled around the level of 70% correct in analysis to avoid situations such as constantly high or low confidence ratings because the task is easy or difficult. The level of 70% correct response was chosen to ensure participants stayed motivated in these trials and they also made enough errors to calculate reliable SDI scores at the same time.

![Classic Type 2 measure and SDI](Kanai_Walsh_SDI.png)

They found that SDI is significantly higher in attentional difficulty conditions compared to sensory difficulty conditions. In other words, participants can accurately judge whether their choice is a correct rejection or a miss when the difficulty arise as a result of increasing attentional demand whereas participants cannot distinguish a miss from correct rejection when the difficulty arise as a result of lacking sensory input. Therefore, the source of noise in perceptual tasks is an important consideration for observers to make confidence judgements; confidence is often adjusted accordingly when noises arises from one’s internal cognitive capacity such as lack of attention whereas the impact of the physical property/environment of perceptual stimuli is less recognised by observer.

![ROC analysis of SDI for group data under visibility suppression.Adapted from Kanai and Walsh (2010)](Kanai_Walsh_SDI_performance.png)


![Metacognition scores for detection and categorization tasks.Adapted from @kanai2010cc.](Kanai_Walsh_Metacognition.png)

More recently, Meuwese et al. (2014) found that metacognitive ability is higher in categorization/ discrimination task than in the detection task for both masked and degraded stimulus. All participants performed the detection and discrimination task, which require them to identify the presence of an animal (e.g. Was there an animal present?) or identify the category (e.g. Was the animal a cat?). The stimuli were either masked by textured patterns or degraded by means of phase scrambling. Participants were then asked to rate how confident they were about the correctness of the choice they made on a scale from 1 to 6. The objective performance in detection and discrimination were also matched at the level of 71% correct to control for the potential effect of objective performance on confidence judgement. Two measures were included for metacognition. One of them is the classic measure of metacognitive sensitivity, the area under the ROC curve. This measure reflects the consistency between subjective confidence ratings of responses made and the actual, objective performance. In addition, they also included the measure of Subjective Discriminability of Visibility (SDV), which includes only trials of hits and false alarms (i.e. trials in which participants reported a stimulus was present), and corresponding Subjective Discriminability of Invisibility (SDI), which includes only trials of miss and correct rejections (i.e. trials in which participants reported a stimulus was absent).

They found that metacognitive sensitivity, according to the classic measure, is higher for discrimination than detection task (Fig.3). Participants can more accurately evaluate the correctness of their choice when the judgement is about which category the target belongs to than when the it is about whether a target was present. Further analysis revealed that such metacognitive superiority does not exist across whole discrimination task and was in fact, specific for correct rejections, measured by SDI. To further unpack the elements driving the differences between detection and discrimination, the metacognitive sensitivity for hits, misses, correct rejections, and false alarms were calculated separately. They found that lower metacognitive accuracy in detection compared with discrimination is driven by lower metacognitive accuracy for correct rejections, i.e. participants subjective judgements of their choice being correct is less accurate for an absence decision when it is physically not there compared with a choice of the stimuli not belonging to the target group when it actually doesn’t. Their results revealed several interesting aspects of confidence formations in these two tasks; people tend to have worse insight for detection than discrimination task, but only in situations where they have correctly rejected the target.

Consistent with findings in Meuwese et al. (2014), Ariel Ezylberberg et al. (2012)found that observers were only influenced by sensory evidence in favour of their decision when forming confidence judgments, while the sensory evidence for the unselected choice does not impact such judgments. Although this is only investigated in two discrimination tasks, luminance comparison and random dot motion task in their study, it might provide an explanation for the poor metacognitive performance in detection task. Consider the situation where participants have made a correct “absence” decision, their confidence rating would be in theory be strongly influenced by evidence in favour of this choice according to the findings from Zylberberg. Hitherto, the lack of sensory evidence in these ‘absence’ trials might underlie a disturbance to a conventional confidence formation process that is engaged in present trials or discrimination trials. Although it is unclear what the consequences of such disturbance are, e.g. does this give rises to the employment of alternative processes such as counterfactual reasoning (discussed in section 2.4 below) specific for absent trials, the uniqueness of confidence formation in absent trials in detection is clear. Therefore, given the finding that participants were strongly influenced by the evidence for selected choice, the fact that in detection, virtually no sensory evidence is available in absence trials to support confidence judgements posts a disadvantage for metacognitive judgements. 

Taking together evidence discussed so far, these studies have shown that making subjective judgements in situations deciding which category the target belongs to is simpler than cases where something is not there at all. 

## 2.3. Neural basis of confidence formation
What brain regions support the formation of metacognitive judgements? Developments in neuroimaging method allow us to reveal the neural underpinnings of confidence formation in both animals and human. Studies have shown rostrolateral and dorsolateral prefrontal cortices (PFCs) are key brain areas involved in confidence judgements in human (Fleming and Doland; see Fleming and Dolan 2012 for a review).
However, as metacognitive judgements often co-occurs with decision-makings, some researchers suggest that confidence judgements share a neural system employed by decision making, while others presented evidence in favour of an independent neural network specific for metacognitive judgements (Qiu et al., 2018). Qiu and colleagues thus conduted a study to loacte the neural systems supporting metacognition independent of decision-makings, in which they testified an independent network in the PFC supporting metacognition. 

Given the behavioural studies described in previous section, showing different confidence judgement formation in discrimination and detection, it is reasonable to expect distinctive neural representations supporting these judgements. Mazor, Friston, and Fleming (2020) investigated the neural contributions to confidence judgements in detection and discrimination. Participants were asked to judge whether a grating was present (detection) or the orientation of a grating (discrimination), after which they reported their confidence ratings. They found distinct neural representations of confidence in detection and discrimination. Specifically, a more pronounced quadratic effect of confidence was observed in the frontopolar cortex (FPC) in the detection task compared with the discrimination task. This provides neural data to demonstrate that making subjective judgements differ between detection and discrimination. As discussed before, this is likely to be due to a unique subjective evaluation process related to absence trials. 
Previous research in metamemory also contributes to the discussion of making confidence judgements about absence. Miyamato et al. (2018) found that area 10 is also recruited for metacognitive evaluation of non-experienced events in macaque monkeys and the frontopolar cortex plays a causal role to confer subjective experiences of non-experienced events.  This finding reflects the possibility that the frontopolar cortex is involved in making confidence judgements of absence since absence can be regarded as ‘non-experience of the stimulus’, which would reflect a qualitative distinctiveness of absence and hence detection. A neural response that increases prior to perceptual disappearance
In this study, the researchers proposed two different approaches to explain this pattern which will be discussed below.

## 2.4. Models of confidence formation
### 2.4.1.SDT Model for confidence formation
Another explanation based on Signal Detection Theory (SDT) concerns the quantitative differences in these tasks. In a detection task, the sensory evidence for two competing choices is unequally distributed, with a greater variance in the presence trials than the absence trials (Fig.1b) as theoretically no sensory stimulus is available in absence trials. In contrast, in a typical discrimination task the two alternatives are equally distributed (Fig.1a). According to the likelihood-ratio calculation of decision making for a binary task, log((l(h1))/(l(h2))) ,equal distribution implies a linear function of log likelihood-ratio calculation in comparison to a quadratic function from an unequally distributed sample (Wickens 2002). Therefore, a more pronounced quadratic effect of confidence in FPC in the detection task might arise as a result of activation in this area representing the relative likelihood of committing to ‘Yes’ or ‘No’ responses drawing from unequally distributed evidence, whereas it is linear when neurons were representing the relative likelihood of ‘Left’ or ‘Right’ drawing from equally distributed evidence. In line with this model, a discrimination task with the attributes of unequally distributed evidence for two alternatives as that in a detection task would also lead to similar quadratic effects of confidence modulation in FPC.
Similar first order model was put forward before to explain the process of forming subjective reports based on SDT and Bayesian inference (King & Deane, 2012). They consider second order judgements to be made based on the same inferential process as the first order judgments that is influenced by the strength of the sensory stimuli. According to this model, the content of conscious perception is the outcome of an inferential decision process, influenced by the stimuli inputs, with a rich set of different output classes that have incorporated subjective judgements such as confidence, rather than a simple present or absent decision (see fig.4 below). The common ground for those first order models that distinguish them from second order models (e.g. counter-factual reasoning model) is that subjective reports, such as confidence judgements were made through the same level of cognitive process as first order judgements without involving higher order state space .

![A multi-dimensional decision-theory framework for objective discrimination and subjective reports. From King and Dehaene, 2014](King_and_Dehane.png)

### 2.4.2. Bayesian models 
The interest in studying confidence formation have extended to developments of computational models. It is widely accepted that human confidence report reflects a Bayesian probabilistic estimation of a choice being correct. To test this assumption, Adler and Ma (2018) compared the fit of several models for forming confidence including the Bayesian model and Non-Bayesian alternatives (detailed below) using two categorisation tasks. The tasks essentially require the participants to report whether the stimulus shown is a visual grating or ellipse, as well as the level of confidence in their choices simultaneously in one press. The contrast of the grating and elongation of the ellipse were manipulated to give rise to the measure of stimuli reliability and corresponds to the level of sensory uncertainty. Their study included two variations of the task such that in task A the two types of stimuli are drawn from Gaussian distributions with different means but same SD, whereas in task B the stimuli are drawn from Gaussian distributions with different SD but same means. 
The consequence of such manipulation is that in task A, stimuli around tilted to certain direction are more likely to be from gabor category, whereas in task B, stimuli around the horizontal are more likely to be from the gabor category (See figure below).

![Experimental design from Adler and Ma (2018) ](Adler and Ma.png)

An ideal Bayesian observer would make a decision by computing a posterior probability of choosing category c given measurement x, p(c|x), which is equivalent to the positive log posterior ratio d=(p(c=1|x))/(p(c=1|x)). The differences in the structure of task A and B have an impact on how Bayesian observers make decisions based on computing this posterior probability such that in task A, participants report stimuli belongs to category 1 when measurement x is positive whereas in task B participants needs both x and trial-by-trial sensory uncertainty to make optimal decisions. 
They found that participants’ confidence reports do not reflect a probability of a choice being correct in a completely Bayesian way but rather a heuristic approximation of this choice being correct. Subjects however do take into account the sensory uncertainty, the perquisite of Bayesian reasoning. Model fit results suggested that participants use the knowledge of their sensory uncertainty over a category, without computing a Bayesian posterior distribution described above but rather basing their response on maximum posteriori estimate of orientation (using the mixture of the two stimulus distributions as a prior distribution).  Models that do not include sensory uncertainty, e.g. fixed models provide a poor fit for the data. Bayesian model does not fully account for the behavioural results and among all models, heuristic models incorporating uncertainty in a non-Bayesian way is a better fit for their results, which suggests that observers do take into account of uncertainty when forming confidence judgements but in a non-Bayesian manner. 

The two tasks used in this study resembles a detection and discrimination tasks from the perspective that detection also involves unequally distributed sensory evidence for two alternatives as that in task B and discrimination entails equally distributed evidence as that in task A.

### 2.4.3. Counterfactual reasoning models
One explanation for the findings from Mazor, Friston, and Fleming (2020) concerns the qualitative differences between detection and discrimination, and more specifically, the unique role of counterfactual reasoning and its interaction with one’s attentional state in detection. Due to the lack of sensory input for absence trials, participants might instead refer to their perceived likelihood of having detected a hypothetical target i.e. the counterfactual situation, in order to make confidence rating for target absence. According to this account, the differences of FPC activations in detection and discrimination might reflect a role of this area in referring to an internal hypothetical state. This process is expected to be influenced by their current state of attention: participants are likely to give low confidence ratings when they evaluate themselves as less attentive as their judgements about the possibility of the counterfactual situation is less certain and vice versa. Behavioural results have supported a role of attention monitoring in subject awareness of performance (Kanai, Walsh, and Tseng 2010). People seem to be able to distinguish their correct rejections from misses when the task difficulty arises from competing attention resources in contrast to when it arises from lack of sensory input. In discrimination task, it is also found that uncertainty related to attention influences observers’ categorization performance and confidence rating in a Bayesian style (Denison et al. 2018). Neuroimaging data also suggested a role of attention in judging subjective experiences in a perceptual filling-in task where participants confidence rating was negatively correlated with attention. Therefore, the process of a counterfactual reasoning, under the influence of monitoring internal states, is possible in the absence trials for detection.

However, it is important to note here that such counterfactual model incorporating attentional control could only account for distinct activations of PFC in ‘No’ responses whereas Mazor, Friston, and Fleming (2020) found similar pattern in ‘Yes’ trials as well. Further investigations are therefore needed to confirm the role of counterfactual reasoning in detection and discrimination.

### 2.5. The current study 
To untangle the counterfactual and SDT models described above, the current study includes another discrimination task with unequal distributions (Fig.4C) in addition to a detection and a discrimination task that are typically used in psychophysics. This will allow us to separate the quantitative feature of the task, i.e. type of distribution, from the qualitative ones, i.e. type of task. We propose an experiment paradigm including three perceptual tasks while participants’ brain activations are being monitored in an MR scanner: A). discrimination task with equal variance (whether gratings are tilted to the right or left; B). detection task with unequal variance (whether a grating is present or not); C). discrimination task with unequal variance (whether a stimulus is a vertical grating or tilted towards any direction). Participants are asked to rate their confidence about their decision following each trial. If quadratic effects of confidence on brain activations are observed in both the discrimination task with unequal variance and the detection task, it is likely that neurons in this area represent the relative likelihood of two competing choices in perceptual tasks as predicted by the SDT model. On the other hand, if the discrimination task with unequal variance shares a similar linear activation profile with the equal-variance discrimination task, it would suggest that the difference in neural representations of detection and discrimination is driven by some more qualitative features of detection, such as counterfactual reasoning proposed in current study. 

![Schematic representation of the experimental procedure](experimental_procedure.png)

## 2.6. Aims and Hypothesis
The current study aims to study how human form confidence in detection and discrimination tasks both from behavioural and neural perspectives. Specifically, this project aims to:

* Compare the confidence formation pattern of the new tilt recognition task with detection and discrimination.

* Replicate the finding from [@mazor2020e] that an interaction between task (discrimination/detection) and the quadratic effect of confidence, in medial and lateral frontopolar cortex, as well as in the STS and preSMA.

* Replicate the finding from [@mazor2020e] of an interaction between detection response (present/absent) and the linear effect of confidence in the right TPJ. 

* Compare the quadratic effect of confidence in the tilt-recognition task with the quadratic effect of confidence in the detection and discrimination tasks in the frontopolar cortex, the STS and the pre-SMA.

* Compare the response-specific linear effects of confidence in the tilt- recognition task with the response-specific linear effects of confidence in the detection and discrimination tasks in the right TPJ.

# 3.Method 
## 3.1. Overview
The current study involved two parts: a behavioural training session lasting around 60 mins and a scanning session lasting around 90 mins with intervals no longer than two weeks. Both parts of the experiments took place at the Wellcome Centre for Human Neuroimaging, University College London. The scanning session was conducted in the 3 Tesla MRI scanner. The ethics of the current study was approved by XX. 

## 3.2. Participants
XX healthy participants were recruited, xx completed the behavioural training session and xx completed the scanning session. According to the preregistration exclusion criteria (see section 3.6. below for details), in total data from 25 participants were included in the final analysis. Participants received cash payments as compensation for their time, £10 for the behavioural session and £20 for the scanning session. To motivate participants to perform their best in our tasks, we also offered bonus payment for good performance and accurate confidence ratings (see procedure below for details on bonus calculation).  
 
## 3.3. Experimental Procedure 
### 3.3.1. Behavioural session
During behavioural training session, participants first received introductions of the study, including general procedure, ethic and data protection protocols. The structure of the three tasks (see fig.4 for the schematic representation) were explained to participants as following:

* Detection task: On half of the trials, a noisy grating will appear after the fixation cross and on the other half there would be no grating shown and you need to decide whether there was a grating present.

* Discrimination task: A grating will appear on the screen every few seconds after the fixation cross, which will be tilted clockwise in half of the trials and anticlockwise in other half. Participants were asked to decide which direction the grating was tilted to. 

* Tilt recognition task: A grating will appear on the screen every few seconds after the fixation cross, which will be tilted (to any direction) in half of the trials and vertical in other half. Participants were asked to decide whether the grating was tilted or vertical. 

* Confidence rating: In all three tasks, immediately after making a choice, you need to indicate how confident you are in your decision by changing the size of the circle.

This session contains a practice block, a calibration block and several training blocks for all three tasks. The response mapping will be counterbalanced between blocks, such that an index finger press will be used to indicate a clockwise tilt on half of the trials, and an anticlockwise tilt on the other half. Similarly, in half of the tilt recognition trials the index finger will be mapped to a vertical response, and on the other half to a tilted response. Lastly, in half of the detection trials the index finger will be mapped to a yes (‘target present’) response, and on the other half to a no (‘target absent’) response. To avoid size-related effect on confidence rating, participants were divided into two groups such that for half participants bigger circle corresponds to higher confidence level and for the other half smaller circle corresponds to higher confidence level.

During this session, each participants performance was controlled around 70 % accurate, by manipulating the task difficulty independently for the three tasks. This will be achieved by using the common 1 up 2 down staircase procedure on stimulus visibility (discrimination and detection task) and on the standard deviation of the orientation distribution (tilt recognition). Participants were not invited back to continue the scanning session if: 1.) their accuracy were lower than 60% or higher than 80%; 2.) had strong response bias, i.e. used the same response in more than 80% of the trials; 3.) had strong confidence bias, i.e. the same confidence level was reported for more than 90% of the trials. 

### 3.3.2. Scanning session
The structure of the three tasks were the same as behavioural session. To motivate participants perform we offered bonus in addition to the baseline payment for the scanning session.  Bonus is calculated use following rule:		
bonus=£$\frac{\overrightarrow{accuracy}.\overrightarrow{confidence}}{200}$. Where $\overrightarrow{accuracy}$ is a vector of 1 and -1 for correct and incorrect responses, and $\overrightarrow{confidence}$ is a vector of integers in the range of 1 to 6, representing confidence reports for all trials. The rule for bonus calculation was explained to participants in both sessions. The scanning session started with a calibration phase to further calibrate participants performance during which time the structural scan for each participant was also obtained. At scanning, 10 discrimination and detection blocks were presented in 5 scanner runs. 

## 3.4. Stimulus
After a temporal rest period of 500-4000 milliseconds, each trial will start with a fixation cross (500 milliseconds). The target was then presented on the screen for 500 milliseconds. In all three conditions, stimuli will consist of 10 grayscale frames presented at 20 frames per second within a circle of diameter 3°. Stimuli will be generated in the following way:

* Generate 10 grayscale frames ( _F_ ... _F_ ), each an array of 142 by 142 random luminance values.
* Create a 142 by 142 sinusudial grating ( 24 pixels per period, random phase). The orientation of the grating is determined according to the trial type.
* The grating visibility for frame _i_ is _pi_ = _v_ × _exp_(-$|\textit{i}-5|$/2) with _v_ being the visibility level in this trial (0 for target-absent trials).
* For each pixel in the frame , replace the luminance value for this pixel with the luminance value of this pixel in the grating with a probability of.

## 3.5. Scanning parameters 
Scanning took place at the Wellcome Centre for Human Neuroimaging, London. The structural images were obtained using an MPRAGE sequence (1x1x1 _mm_ voxels, 176 slices, in plane FoV = 256x256 _mm_ 2), followed by a double-echo FLASH (gradient echo) sequence with TE1=10ms and TE2=12.46ms (64 slices, slice thickness = 2 _mm_, gap = 1 _mm_, in plane FoV= 192×192 _mm_ 2, resolution = 3×3 _mm_ 2) that were later used for field inhomogeneity correction. Functional scans were acquired using a 2D EPI sequence, optimized for regions near the orbitofrontal cortex (3.0x3.0x3.0 _mm_ voxels, TR=3.36 seconds, TE = 30 ms, 48 slices tilted by -30 degrees with respect to the T¿C axis, matrix size = 64x72, Z-shim=-1.4).

```{r tidy data, include=FALSE, cache=TRUE}
# open file
data <- read.csv("data.csv")

# creat a new file with only included trials+ transform rt into log_rt
data_valid <- data %>% 
  #Filter out the excluded trials 
  filter(inclusion==1) %>%
  filter(! response=="NaN") %>%
  mutate(rt=response_time*1000) %>%
  mutate(log_rt=log(rt))
data_valid

# save a version of the df with raw response&stimulus code (i.e. 1 and 0)
data_valid_raw <- data_valid

# change response codes
data_valid$response[(data_valid$response=="0") &
                                   data_valid$task=='Detection'] <- "No"

data_valid$response[(data_valid$response=="1") &
                                   data_valid$task=='Detection'] <- "Yes"

data_valid$response[(data_valid$response=="0") &
                                   data_valid$task=='Discrimination'] <-"Anticlockwise"

data_valid$response[(data_valid$response=="1") &
                                   data_valid$task=='Discrimination'] <- "Clockwise"

data_valid$response[(data_valid$response=="0") &
                                   data_valid$task=='Tilt'] <- "Vertical"

data_valid$response[(data_valid$response=="1") &
                                   data_valid$task=='Tilt'] <- "Tilted"

# transform into data.table
dt <- data.table(data_valid)
dt_raw <- data.table(data_valid_raw)

# create a tidy version of data file (BOLD signal as a single vector, ROIs as grouping variable)
data_tidy <- rename(data_valid, c('rTPJ'='BOLD_rTPJ', 'vmPFC'='BOLD_vmPFC', 'FPl'='BOLD_FPl', 'FPm'='BOLD_FPm', 'pMFC'='BOLD_pMFC'))
data_tidy <- gather(data_tidy, "ROI", "BOLD", 16:21)%>% filter(!BOLD=="NaN")

# separate the data for each task respectively
data_det <- data_valid %>%filter(task=="Detection")
data_dis <- data_valid %>%filter(task=="Discrimination")
data_til<- data_valid %>%filter(task=="Tilt")

# get the total number of participants
no_subj <- length(unique(data_valid$subj_id))

# count the total no of trials for each task for each participant
total_trials <- data_valid %>% group_by(subj_id, task) %>% summarise(totaltrials=(count=n()))

#get the number of trials for each participants where the stimulus shown is 1 (i.e.Present, Clockwise & Tilted)
sti_trials <- data_valid %>% group_by(subj_id, task) %>%filter(stimulus=="1")%>%summarise(sti_trials=(count=n()))

#count no. of correct trials for each task for each participant
individual_trials <- data_valid %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id, task, accuracy) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  #combine with the total data frame
  inner_join(total_trials, by = c ("subj_id","task")) %>%
  mutate(proportion = no_trials / totaltrials)

#display accuracy 
individual_accuracy <- individual_trials %>% filter(accuracy==1) %>%rename(correct=proportion)

```

# 4.Results
## 4.1.Performance across threee tasks
```{r , performance analysis, include=FALSE, cache=TRUE}
#display accuracy for each task 
task_accuracy <- individual_accuracy %>% group_by(task) %>% summarise(mean(correct)) %>% spread(key='task', value='mean(correct)')

#ANOVA comparing task performance 
aov_accuracy <- aov_ez(data=individual_accuracy, dv= "correct", within= "task", id= "subj_id")

#################
#calculating d'
#################
#count the number of accurate/inaccurate trials for each response for each task & combine with the sti_trials data frame
trials <- data_valid %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id, task, accuracy, response) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  inner_join(sti_trials, by = c ("subj_id","task"))

# transform trials into data table and include only positive responses for three tasks
dt_trials  <- data.table(trials)%>% filter(response=="Yes"| response=="Tilted"| response=="Clockwise")
# calculate the z value for hit and fa 
rate_subj <- dt_trials[, .(rate=no_trials/sti_trials), by=.(subj_id,response, accuracy)] %>% mutate(z_rate=qnorm(rate))  

# drop the rate column (for later 'spread' function to work properly)
rate_subj$rate <- NULL

# spread the file based on accuracy (i.e. HIT and FA respectively)
rate_subj <- spread(rate_subj,key='accuracy', value='z_rate')  
colnames(rate_subj) <- c("subj_id", "response", "z_FA", "z_HIT")

# d prime for each subjects and task
d_subj <- rate_subj[, .(d_prime=z_HIT-z_FA), by=.(subj_id,response)]

# d prime for each task
d_task <- d_subj[, .(mean_d=mean(d_prime)), by=.(response)] 

#ANOVA comparing d prime
aov.d <- aov_ez(data=d_subj,
                id="subj_id",
                dv= "d_prime",
                within = "response")
```

```{r, accuracy_figure, echo=FALSE, message=FALSE,fig.cap="\\label{fig:accuracy} Mean accuracy across three tasks", cache=TRUE}
number_ticks <- function(n) {function(limits) pretty(limits, n)}
ggplot(individual_accuracy, mapping = aes(x= task, y=correct)) + ylim(0.4, 1) +labs (y="mean accuracy")+ theme_classic() +
   geom_boxplot()+ geom_jitter(position=position_jitter(width=.1, height=0), alpha=0.3, size=4) + scale_y_continuous(breaks=number_ticks(6), limits=c(0.4, 1)) + geom_hline(yintercept=0.5, linetype="dashed", color = "black") + theme(plot.title = (element_text(color = "black", size = 12, face = "italic")))
```

The performance across three tasks, detection (accuracy= `r apa(task_accuracy$Detection, 2, T)`, d'= `r apa(d_task$mean_d[d_task$response=='Yes'], 2, T)`),  discrimination (accuracy= `r apa(task_accuracy$Discrimination, 2, T)`, d'= `r apa(d_task$mean_d[d_task$response=='Clockwise'], 2, T)`) and tilt recognition (accuracy = `r apa(task_accuracy$Tilt, 2, T)`, d'= `r apa(d_task$mean_d[d_task$response=='Tilted'], 2, T)`) was similar, where accuracy was computed as the proportion of correct trials out of total number of trials and d’ was calculated using the following formula:  d’ = z(FA) – z(H). A one-way ANOVA failed to detect a significant difference between the accuracy of these three tasks `r apa_print(aov_accuracy)$full_result` and d' `r apa_print(aov.d)$full_result` ; see Figure \ref{fig:accuracy})), which reflected a good control over the difficulty level across the three tasks. 

```{r, include=FALSE, cache=TRUE}
###########################
# calculating response bias 
###########################
response <- data_valid_raw %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id,task, response) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  #combine with the total data frame
  inner_join(total_trials, by = c ("task","subj_id")) %>%
  mutate(prob = no_trials / totaltrials)

# calculate the response bias for each individual
subj_bias <- subset(response, select= c(subj_id, task, response, prob)) %>%
  spread(response, prob) %>% rename("Zero"="0", "One"="1") %>% mutate(bias=One-Zero) %>% subset(select= c(subj_id, task, bias)) %>% spread(task,bias)

# test if participants who are more likely to respond Yes also are more likely to respond Tilted
bias_lm <- apa_print(lm(data = subj_bias, Detection~Tilt)) # the correlation was not significant

# create a function to get the mean response probability +sd and do a t test to compare with 0.5

resp_bias <- function(resp, cur_task, df){
  df <- df %>% filter(response==resp&task==cur_task)
  resp_mean <- mean(df$prob)
  resp_sd <- sd(df$prob)
  task <- (rnorm(no_subj, mean = resp_mean, sd = resp_sd))
  apa_t <- apa_print(t.test(task, mu = 0.5))
  my_list <- list("response"=resp, "mean"=resp_mean, "sd"=resp_sd, "apa_t"=apa_t)
  return(my_list)
}

resp_yes <- resp_bias('1', 'Detection',response)
resp_no <- resp_bias('0', 'Detection',response)
resp_dis <- resp_bias('1', 'Discrimination', response)
resp_til <-resp_bias('1', 'Tilt',response)
resp_ver <-resp_bias('0', 'Tilt',response)

```
## 4.2.Response Bias
To test whether participants were biased towards certain response, we computed the probability of giving each response in three tasks. The probability of responding NO in detection was `r apa(resp_no$mean, 2, T)` (± `r apa(resp_no$sd, 2, T)`), and was  significantly different from 0.5 (`r resp_no$apa_t$full_result`), showing that participants are more likely to give NO responses than YES response in detection task. The probability of responding CLOCKWISE was `r apa(resp_dis$mean, 2, T)` (± `r apa(resp_dis$sd, 2, T)`) and was not significantly different from 0.5 (`r resp_dis$apa_t$full_result`). For the tilt recognition task, the probability of responding TILTED was `r apa(resp_til$mean, 2, T)` (± `r apa(resp_til$sd, 2, T)`), which was significantly lower than responding VERTICAL `r apa(resp_ver$mean, 2, T)` ± (`r apa(resp_ver$sd, 2, T)`) (`r resp_til$apa_t$full_result`).

Furthermore,given the existence of significant response bias in both detection and tilt recognition task, statistical test was conducted to assess whether individuals who are more likely to give NO responses in detection are also more likely to give VERTICAL response in tilt recognition. A linear model was fitted to the probabilities of giving these two response within each individual and this model fit result suggest that this relationship was not significant (`r bias_lm$full_result`). 

```{r, include=FALSE, cache=TRUE}
#####################
#calculating rt
#####################
# get individual median rt, mean rt, and mean log rt for different accuracy
subj_rt_acc <- dt[, .(median_rt=median(rt), mean_rt= mean(rt), mean_log_rt=mean(log_rt)), by=.(subj_id,task,accuracy)]

# get group median rt
rt_acc <- subj_rt_acc[, .(median_rt=median(median_rt), mean_log_rt= mean(mean_log_rt)), by=.(task, accuracy)]

# get quantiles of rt for different accuracy 
qt_accuracy <- rt_acc[, .(quantile(median_rt, c(.25,.5,.75))), by=accuracy]

#t test comparing log_rt for correct and incorrect response
t_rt_acc <- t.test (mean_log_rt~accuracy, mu=0, paired=TRUE, data=subj_rt_acc)

# get individual median rt, mean rt, and mean log rt
subj_rt <- dt[, .(median_rt=median(rt), mean_rt= mean(rt), mean_log_rt=mean(log_rt)), by=.(subj_id,task,response)]

# get group median rt
rt <- subj_rt[, .(median_rt=median(median_rt), mean_log_rt= mean(mean_log_rt)), by=.(task, response)]

#t test comparing the effect of response type on log_rt for detection, discrimination and tilt 
t_rt_det <-t.test(mean_log_rt~response, mu=0, paired=TRUE, data=(subj_rt%>% filter(task=='Detection')))
t_rt_dis <-t.test(mean_log_rt~response, mu=0, paired=TRUE, data=(subj_rt%>% filter(task=='Discrimination')))
t_rt_til <-t.test(mean_log_rt~response, mu=0, paired=TRUE, data=(subj_rt%>% filter(task=='Tilt')))

```

## 4.3.Response time
Consistent with previous studies, response time was faster for correct response (1st quantile= `r apa(qt_accuracy[1,2]$V1, 2, T)`, median= `r apa(qt_accuracy[2,2]$V1, 2, T)`, 3rd quantile= `r apa(qt_accuracy[3,2]$V1, 2, T)` milliseconds) than incorrect responses (1st quantile= `r apa(qt_accuracy[4,2]$V1, 2, T)`, median= `r apa(qt_accuracy[5,2]$V1, 2, T)`, 3rd quantile= `r apa(qt_accuracy[6,2]$V1, 2, T)` milliseconds) according to a paired t test on the log reaction time(`r apa_print(t_rt_acc)$full_result`). No significant overall effect of responses type on log reaction time in detection task (YES vs. NO, `r apa_print(t_rt_det)$full_result`), discrimination task (CLOCKWISE vs. ANTICLOCKWISE, `r apa_print(t_rt_dis)$full_result`) and tilt recognition task `r apa_print(t_rt_til)$full_result`) were detected according to the results from paired sample t tests. 

```{r, log_rt_figure, echo=FALSE, message=FALSE, fig.cap="\\label{fig:log_rt} Log transformed reaction time across three tasks", cache=TRUE}
# get mean log rt at each confidence level for each subject
subj_log_rt <- dt[, .(mean_log=mean(log_rt)), by=.(subj_id, task, response, confidence)]

# get mean log rt at each confidence levels for each task 
log_rt <- subj_log_rt[, .(log_rt=mean(mean_log), log_rt_se=se(mean_log)), by=.(task, response, confidence)]
color_scheme <- tibble(Detection=c("#e41a1c", "#377eb8"), 
                       Discrimination=c("#984ea3", "#4daf4a"),
                       Tilt=c("#999999","#f781bf"))

cbPalette <- c("#984ea3", "#4daf4a","#e41a1c", "#f781bf","#999999","#377eb8")

# plotting log rt against confidence
log_rt_plot <- ggplot(data=log_rt, aes(x=confidence, y=log_rt)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = log_rt-log_rt_se, ymax = log_rt+log_rt_se, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=cbPalette) +
  labs(title='Log reaction time and confidence', x= "confidence", y="mean Log RT ") +
  theme_classic() + 
  scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + 
  theme(legend.background = element_blank(),
        legend.key = element_blank(),
        legend.title = element_blank(),
        legend.text = (element_text(size = 10, face = "bold")),
        axis.text=(element_text(size=14)),
        axis.title=(element_text(size=14,face="bold")),
        plot.title = (element_text(color = "black", size = 14, face = "bold.italic")))+
facet_wrap(~task)

print(log_rt_plot)

```

```{r, log_rt_accuracy figure, echo=FALSE, message=FALSE, fig.cap="\\label{fig:rt_accuracy tradeoff} ", cache=TRUE}
########################################
# speed accuracy trade-off scatter plot
########################################
rt_test <- dt_raw[, .(median_rt=median(rt)), by=.(subj_id,task, response)]%>%spread(key = 'response', value = 'median_rt') %>%rename(present="1", absent="0") %>% mutate(median_rt=present-absent) %>%subset(select=c(subj_id, task, median_rt))

rt_acc <- data_valid_raw %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id,task, accuracy, response) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(acc_resp_trials=(count = n())) %>%
  #combine with the total data frame
  inner_join(response, by = c ("subj_id","task", "response")) %>%
  mutate(correct = acc_resp_trials / no_trials) %>%
  filter(accuracy==1) %>%
  subset(select=c(subj_id, task, response, correct)) %>%
  spread(key = 'response', value = 'correct') %>%
  rename(present="1", absent="0") %>%
  mutate(acc=present-absent) %>%
  subset(select=c(subj_id, task, acc)) %>%
  inner_join(rt_test, by = c ("subj_id", "task"))

rt_accuracy_plot <- ggplot(data=rt_acc, aes(x=median_rt, y=acc)) + 
    geom_point(alpha=0.4, size=4)+
    geom_smooth(method=lm, se=FALSE, linetype="dashed",
             color="darkred")+
    theme_classic()+
    labs(title= "Reaction time and accuracy", x= "mean reaction time difference between responses", y="mean accuracy difference between responses")+
  facet_wrap(~task)

print(rt_accuracy_plot)

rt_acc_det <- apa_print(lm(data = (rt_acc %>% filter(task=='Detection')), acc~median_rt))
rt_acc_dis <- apa_print(lm(data = (rt_acc %>% filter(task=='Discrimination')), acc~median_rt))
rt_acc_til <- apa_print(lm(data = (rt_acc %>% filter(task=='Tilt')), acc~median_rt))

```

To explore the relationship between reaction time and accuracy in each task, further linear regression analysis was carried out. Namely, we are interested in whether participants who took longer to give one response (YES, CLOCKWISE and TILTED) than the opposite (NO, ANTICLOCKWISE and VERTICAL) are also more likely to be correct in those choices. We computed the accuracy difference and response time difference between each response pairs in three task and tested the linear relationship between them, the results from which reveals a significant correlation in discrimination `r rt_acc_dis$full_result` but not in detection `r rt_acc_det$full_result` and tilt recognition `r rt_acc_til$full_result` (see Figure \ref{fig:rt_accuracy tradeoff}). 

```{r include=FALSE, cache=TRUE}
#confidence frequency for each task & response for each subject
subj_conf <- data_valid %>%
  group_by(subj_id, task,confidence,response) %>%
  summarise(conf_trials=(count=n())) %>%
  as.data.table()

# group-level confidence frequencies
confidence_distribution <- subj_conf[, .(frequency=mean(conf_trials), se=se(conf_trials)), by=.(task, response, confidence)]

mean_conf_sub <- dt[, .(confidence=mean(confidence)), by=.(subj_id, task, response)]
mean_conf_group <- mean_conf_sub[, .(confidence=mean(confidence)), by=response]

conf_til <- mean_conf_group$confidence[mean_conf_group$response=="Tilted"]
conf_ver <- mean_conf_group$confidence[mean_conf_group$response=="Vertical"]

# t test comparing confidence levels between two responses in each task
t_conf_det <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, paired=TRUE, data = (mean_conf_sub %>% filter(task=="Detection")))

t_conf_dis <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, paired=TRUE,
                     data = (mean_conf_sub %>% filter(task=="Discrimination")))

t_conf_til <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, paired=TRUE,
                     data = (mean_conf_sub %>% filter(task=="Tilt")))

data_valid$response[(data_valid$response=="0") &
                                   data_valid$task=='Detection'] 
```
&nbsp;

## 4.4.Confidence distributions
Within detection, no significant difference in mean confidence was observed between Yes (target present) and NO (target absent) responses (see Fig.4 above) (`r apa_print(t_conf_det)`) and similarly, in the discrimination task participants' mean confidence ratings did not differ significantly between the CLOCKWISE and ANTICLOCKWISE responses (`r apa_print(t_conf_det)`). However the tilt recognition task, a paired sample t test revealed that the effect of response type had a significant impact on mean confidence levels (`r apa_print(t_conf_til)`, such that participants are more confident in TILTED (` r apa(conf_til, 2, T)`) than VERTICAL response (` r apa(conf_ver, 2, T)`) (see Figure \ref{fig:confidence}). 

```{r echo=FALSE, message=FALSE, fig.cap= "\\label{fig:confidence} Confidence distributions across three tasks and responses. Error bars represent the standard error of the mean.", cache=TRUE}
#plot the frequency of confidence ratings for all three tasks
task_labs = c('Detection','Discrimination','Tilt Recognition')
names(task_labs) = c('Detection','Discrimination','Tilt')

confidence_det <- ggplot(confidence_distribution, mapping = aes(x = confidence, y= frequency, fill= response)) +
  geom_bar(stat="identity", width = 0.65, position = position_dodge(width = 0.7))+
  geom_errorbar(aes(ymin=frequency-se, ymax=frequency+se), position=position_dodge(.7), width=0.2)+ 
  scale_fill_manual(values=cbPalette) + 
  theme_classic() + 
  labs(title="Confidence distribution by task and response", y="frequency")+ scale_x_continuous(breaks=number_ticks(6)) +ylim(0,30)+
  theme(plot.title = (element_text(color = "black", size = 12, face = "italic")))
confidence_det+facet_grid(rows=vars(task),
                          labeller = labeller(task=task_labs))

```
&nbsp;

&nbsp;

``` {r Type 2 ROC , echo=FALSE, message=FALSE, }
#get the total number of trials for correct vs incocrrect for each response respectively 
acc <- data_valid %>%
  group_by(subj_id, task, accuracy, response) %>%
  summarise(acc_trials=(count=n())) 

#get the total number of trials for correct vs incoccrect for each response respectively at each confidence rating
conf <- data_valid %>%
  group_by(subj_id, task, accuracy, response, confidence) %>%
  summarise(conf_trials=(count=n())) %>%
  inner_join(acc, by = c ("subj_id","task", "response", "accuracy")) %>%
  # calculate the prob of giving each confidence rating in that response (correct vs. inccorect) for each subject
  mutate(prob=conf_trials/acc_trials) 

#calculate individual AUC
# filling in the missing values for each combination (task, accuracy, response and confidence, i.e.where ppl didn't give all conf ratings)
conf_subj <- conf %>% ungroup %>% complete(subj_id, accuracy, response, confidence = 1:6, fill = list(conf_trials = 0, prob = 0)) %>% group_by(subj_id, task, accuracy, response, confidence)

# convert into data table
conf_subj <- as.data.table(conf_subj)
# sort the df with descending confidence level
conf_subj <- conf_subj[order(-confidence),]

# get the cumulative prob for each task & response & accuracy for each subject 
cum_prob_subj <- conf_subj[, .(cum_prob=cumsum(prob), confidence), by=.(subj_id, response, accuracy)] %>% spread(key=accuracy, value=cum_prob)%>% rename("correct"="1", "incorrect"="0") %>%   #add the origin(0,0) point for each ROC plot
  complete(nesting(subj_id, response), confidence = 1:7, fill = list(correct=0, incorrect=0)) %>% #add a column specifying task
  mutate(task = case_when(response == "Clockwise" ~ "Discrimination", 
                                response == "Anticlockwise" ~ "Discrimination",
                                response == "Yes" ~ "Detection",
                                response == "No" ~ "Detection",
                                response == "Tilted" ~ "Tilt",
                                response == "Vertical" ~ "Tilt")) %>%as.data.table()

# calculating area under the curve for each response for each subject
# AUC_subj <- as.data.frame(sapply(split(cum_prob_subj, list(cum_prob_subj$subj_id, cum_prob_subj$response)), function(x) {data.table(auc=auc(x=x$incorrect, y=x$correct, from=1), rownames=x$subj_id)
# }))

# calculate the mean prob & se for the group
cum_prob <- cum_prob_subj[, .(incorrect=mean(incorrect),correct=mean(correct), incorrect_se=se(incorrect), correct_se=se(correct)), by=.(task, response, confidence)] %>% complete(nesting(task, response), confidence = 1:7, fill = list(correct=0, incorrect=0, correct_se=0, incorrect_se=0))

# calculate the AUC for the group 
AUC <- as.data.frame(sapply(split(cum_prob, cum_prob$response), function(x){
  data.table(auc=auc(x=x$incorrect, y=x$correct, from=0))
})) 

# add AUC values to responses (to appear on legend in ROC plots)
cum_prob$response[cum_prob$response=="Vertical"] <- paste0('Vertical (AUC=',apa(AUC$Vertical.auc, 2, T), ')')
cum_prob$response[cum_prob$response=="Tilted"] <- paste0('Tilted (AUC=',apa(AUC$Tilted.auc, 2, T), ')')
cum_prob$response[cum_prob$response=="Yes"] <- paste0('Yes (AUC=',apa(AUC$Yes.auc, 2, T), ')')
cum_prob$response[cum_prob$response=="No"] <- paste0('No (AUC=',apa(AUC$No.auc, 2, T), ')')
cum_prob$response[cum_prob$response=="Clockwise"] <- paste0('Clockwise (AUC=',apa(AUC$Clockwise.auc, 2, T), ')')
cum_prob$response[cum_prob$response=="Anticlockwise"] <- paste0('Anticlockwise (AUC=',apa(AUC$Vertical.auc, 2, T), ')')

```

## 4.5.Metacognitive sensitivity
Metacognitive sensitivity, which is quantified as the area under Type 2 ROC curve (see \ref{fig:type2 ROC} ), is significantly higher for Yes (`r apa(AUC$Yes.auc, 2, T) `) than No (`r apa(AUC$No.auc, 2, T)`) response (T RESULTS). Similar pattern is observed in the tilt recognition task, where the AUC is significantly higher for Tilted (`r apa(AUC$Tilted.auc, 2, T)`) than Vertical (`r apa(AUC$Vertical.auc, 2, T)`) responses (T RESULTS) whereas AUC is similar between the two responses in discrimination task. This suggests that participants confidence ratings are more diagnostic to accuracy in the judgments of a target stimulus being present than being absent. Similarly, the correct judgments of a stimulus being tilted is better reflected by participants' confidence ratings than the correct judgments of a stimulus being vertical. 

```{r echo=FALSE, message=FALSE, fig.cap="\\label{fig:type2 ROC} Type 2 ROC curve for each task.", cache=TRUE, fig.width=3.2, fig.height=3.2}
#################################### 
# Plotting type 2 ROC
####################################
roc_plot <- function(cur_task, df){
  
  # only include data for current task
  df <- df%>%filter(task==cur_task)
  
  plot<-ggplot(data=df, aes(x=incorrect, y=correct)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar(aes(ymin=correct-correct_se, ymax=correct+correct_se, colour = factor(response))) +
  geom_errorbarh(aes(xmin=incorrect-incorrect_se, xmax=incorrect+incorrect_se,colour = factor(response))) +
  scale_color_manual(values=color_scheme[[cur_task]]) +
  labs(title=paste('Type 2 ROC', cur_task), x= "p(conf | incorrect)", y="p(conf | correct)", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0, 1),breaks=number_ticks(6)) + scale_y_continuous(limits = c(0, 1), breaks=number_ticks(6)) +
  theme(legend.position=c(0.7, 0.15),  
                 legend.background = element_blank(),
        legend.title = element_blank(),
                 legend.key = element_blank(), 
        legend.text = element_text(size=10, face="bold"),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) +
  geom_segment(x = 0, y = 0, xend=1, yend=1, linetype="dashed")
  
  print(plot)
}

for (task in list('Detection','Discrimination','Tilt')) {
    roc_plot(task,cum_prob)
  }

```
&nbsp;

```{r linear effects, echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}
#########################################################
# linear regression (BOLD~confidence) for each individual
#########################################################
dt_subj <- data.table(data_tidy)

beta_subj <- dt_subj[, .(beta=(lm(BOLD~confidence)$coef[2])), by=.(subj_id, task, response, ROI)]

########################################
# compare the slope of lm at group level
########################################
dt_group <- data.table(beta_subj)

# linear effect of confidence on each ROI
beta <- dt_group[, .(beta_group=mean(beta), beta_sd=sd(beta)), by=ROI]

lm_ROI <- function(ROI, df){
  ttest <- (rnorm(no_subj, mean = df$beta_group[df$ROI==ROI], sd = df$beta_sd[df$ROI==ROI]))
  apa_t <- apa_print(t.test(ttest, mu = 0.5))
  return(apa_t)
}

# compare slop between different tasks for each ROI
beta_task <- dt_group[, .(p_task=(summary(aov(beta~task))[[1]][["Pr(>F)"]][[1]])), by=ROI]

# compare slope between different responses within each task for each ROI
beta_resp <- dt_group[, .(p_response=(summary(aov(beta~response))[[1]][["Pr(>F)"]][[1]])), by=.(task, ROI)]

# extracting beta for det and FPl
FPl_det <-(beta_resp$p_response[beta_resp$task=='Detection'&beta_resp$ROI=='FPl'])
```
## 4.5.Effect of confidence on BOLD signal in ROIs
From our data, negative linear confidence-related effects were observed in right Temporoparietal Junction (rTPJ) ($\beta$=`r apa(beta$beta_group[beta$ROI=='rTPJ'],2,T)`, `r lm_ROI('rTPJ', beta)`), Posterior Medial Frontal Cortex (pMFC)($\beta$=`r apa(beta$beta_group[beta$ROI=='pMFC'],2,T)`, `r lm_ROI('pMFC', beta)`), as well as positive linear correlation between confidence and BOLD signals in Ventromedial Prefrontal Cortex (vmPFC)($\beta$=`r apa(beta$beta_group[beta$ROI=='vmPFC'],2,T)`, `r lm_ROI('vmPFC', beta)`), Medial Frontopolar Cortex (FPm)($\beta$=`r apa(beta$beta_group[beta$ROI=='FPm'],2,T)`, `r lm_ROI('FPm', beta)`) and Lateral Frontopolar Cortex (FPl)($\beta$=`r apa(beta$beta_group[beta$ROI=='FPl'],2,T)`, `r lm_ROI('FPl', beta)`). 

To investigate whether linear effects of confidence on BOLD signal were influenced by the type of task (Detection vs. Discrimination vs. Tilt recognition) and/or response (Yes vs. NO; Clockwise vs. Anticlockwise; Vertical vs. Tilted), linear models with interactions were tested. The effects of confidence failed to show a significant difference between three tasks in all ROIs (all p values > 0.32). Significant effects of response type were observed in the FPl region for both detection ` r apa(FPl_det, 2, T)` and discrimination `r apa(beta_resp$p_response[beta_resp$task=='Discrimination'& beta_resp$ROI=='FPl'], 3, T)` as well as a significant difference between CLOCKWISE and ANTICLOCKWISE response in rTPJ `r apa(beta_resp$p_response[beta_resp$task=='Discrimination'& beta_resp$ROI=='rTPJ'], 3, T)`. No significant effect between different responses was observed for three tasks in other regions (p values > 0.13).

## 4.6.Whole brain analysis
[CG:not sure how to present those results so only describing briefly]

In the whole brain analysis, we found linear relationships between confidence and activation in several ROIs. However, no significant interaction of task type (detection vs. discrimination, tilt recognition vs. discrimination) was found on the quadratic effect of confidence at significant level of <.001. 
![Linear effects of confidence. (73.68, 2.68, 2.68)](confidence.png)
![Quadratic effects of confidence.](confidence2.png)
![Main effect of task. (-6.4, -40.7, 7.15)](task.png)

# 5.Discussion
Detection and discrimination are two distinctive perceptual processes that constitute some of the most fundamental tasks we perform on a daily basis. Detection involves identifying the presence of a target whereas discrimination requires one to categorise the target into certain group. The differences in the nature of those two tasks have been shown to influence the accuracy of one’s confidence judgement towards their performance (XX), with research showing that such judgements are more accurate in discrimination than detection. Differences between detection and discrimination were also manifested in a unique quadratic effect of confidence that were found in rTPJ and frontopolar cortex in detection task in contrast to discrimination task (Mazor, Friston, & Fleming, 2020). The current study aims to investigate whether such unique activation pattern reflects a qualitative difference, i.e. distinctive cognitive processes involved, or a quantitative difference, i.e. equal or unequal distribution of sensory evidence, in forming confidence in those two tasks, between detection and discrimination. To test those different accounts, a novel discrimination task, which at the same time, also fulfils the characteristics of a detection task from the perspective of SDT, was introduced. This novel task includes two alternative choices that have unequal variance in the distribution of sensory evidence. Meanwhile, we also aim to explore any potential difference the patterns participants behave between the new tile recognition task and conventional detection/discrimination tasks.

## 5.1. response bias
In the current study, we found that participants are more likely to give NO response than YES response while they are also more likely to give VERTICAL than TILTED response. The probability of giving two responses in discrimination task is well balanced. Results from the detection task was not consistent from that in Mazor et al., (2020) where they reported no significant bias towards any response. 
Since the new task was designed to resemble the SDT features of the detection task and the fact we found response bias in both, we then conducted exploratory analysis to assess whether participants who had biases toward NO response in detection also share bias in tilt recognition. The response bias in these two tasks was not correlated at an inter-individual level, i.e. participants who are more likely to give NO response did not necessarily tend to give VERTICAL responses. 

## 5.2. metacognitive sensitivity in three perceptual tasks
Level of metacognitive sensitivity, which was quantified as the area under the Type-II ROC curves, was calculated for each individual that was then used for group-level analysis. Consistent with previous findings, AUC is higher for YES than NO responses in detection task. Participants’ metacognitive sensitivity did not differ significantly between the two responses in discrimination, which was also in line with previous findings. We initially hypothesized that the new tilt recognition task would resemble the detection task behaviourally as they share similar features from the perspective of SDT. This hypothesis was supported by our results showing that in the new tilt recognition task also share a discrepancy, that is even wider than detection, in the metacognitive sensitivity between the two responses. 
The lower level of accuracy in metacognitive judgements in absence responses in comparison to presence response has been shown repeatedly by previous studies (e.g. Fleming & Dolan, 2010; Kanai et al., 2010; Mazor, Fleming and Friston, 2020). This discrepancy has been explained mainly from two perspectives. The first attributes it to the stimuli property in absence trials at a basic sensory processing level. Based on SDT, the structure of the detection task entails two unequal distributions for absence and presence choices and furthermore, the widely distributed presence choices would translate into greater AUC in Type-II ROC curves (Maniscalco & Lau, 2011). 

## 5.3. reaction time
Results from our log transformed reaction time analysis showed that participants were generally faster at giving correct responses than incorrect responses, in line with previous results (Mazor, Fleming & Friston, 2020). This regular pattern of decision making is rather straightforward to explain from either direction: correct choices are those easier and thus faster to make or faster choices are more likely to be correct. 
Effect of response type on reaction time was investigated in detection, discrimination and tilt recognition tasks respectively. No significant difference of reaction time and responses pairs (YES/NO, CLOCKWISE/ANTICLOCKWISE and VERTICAL/TILTED) were identified in our analysis. This diverges from the results for detection reported in Mazor, Fleming and Friston (2020), in which they reported that YES responses were significantly faster than NO responses. 
This differences in results were assigned to the variations in task structures, specifically the extension of stimulus presentation window (from 33s to 1500s) in the current study. It is possible that when duration of showing stimulus is long, time taken to make a response is not as meaningful as when duration is short in informing participants decision-making process. This is because long stimulus window potentially allows one to decide during presentation window already, in which case response time captured during designed response window is merely the motor execution of that already-made decision rather than a complex cognitive process.
We also explored the relationship between any potential reaction time and accuracy tard-offs. To investigate whether participants who took longer to make certain response are also those who have higher accuracy level for these responses, we computed the reaction time differences and accuracy difference for each individual between two response types in all three tasks. In discrimination, participants who took longer to give CLOCKWISE responses than ANTICLOCKWISE responses were also more likely to be correct in these judgements. Although this relationship was not statistically significant in the detection and tilt recognition task (which might be due to sample being underpowered, see section 5.4 below), there is some evidence to suggest a positive correlation between reaction time and accuracy in general. This pattern resembles the evidence accumulation model for decision making to certain degree. However, unfortunately a more complete sample is required to study this further. 

# 6. Ethics of animal research
An important consideration we draw readers attention to is the aspect considering ethics of the methodology used in cited animal studies (CITE ALL ANIMAL RESEARCH HERE). These studies often involves training animals (e.g. rhesus monkeys) to perform tasks to serve the purpose of research. However, the details of training process is often not fully reported in published papers, which in a lot of cases involves restraining animals from satisfying their physical needs (e.g. starvation). Although these existing studies do provide different perspectives from human research, here we call researchers to reconsider the ethics of such methods and develop alternative training systems that for instance, emphasis more on rewards without harming basic needs of animals. 

# 7. Reference
