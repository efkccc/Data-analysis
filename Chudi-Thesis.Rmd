---
title: "Chudi Thesis"
author: "Chudi Gong"
date: "18/06/2020"
output: pdf_document
bibliography: references.bib
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(citr)
library(tinytex)
library(vctrs)
library(dplyr)
library(magrittr)
library(ggplot2)
library(readr)
library(tidyr)
library(RColorBrewer)
library(papaja)
library(MOTE)
library(MESS)
library(lmerTest) # for mixed effects model fit
library(broom.mixed) # to tidy model fit coefficients
getwd()

# MM: This function is not needed, papaja handles this :)
myround_p <- function(x, digits = 3){
  if (x < 10^-digits) return(paste('<', 10^-digits))
  paste('=', myround(x, digits))
}

number_ticks <- function(n) {function(limits) pretty(limits, n)}
```
# 1.Abstract

# 2.Introduction 
As human beings we often make statements such as ‘I am confident that…’, reflecting the importance of confidence, which has wide applications in fields such as medical diagnosis, financial choices and eyewitness testimony etc.. Confidence can be defined as ‘a belief about the validity of our own thought, knowledge or performance that relies on a subjective feeling.’ [@grimaldi2015nbr]. The confidence of one decision can bear on another decision, such as when one decides to drop a stock investment after purchasing. The formation of confidence in human perception has been investigated extensively, with developments of various behavioural, neural and computational models.  

Traditionally, research investigating how confidence is formed with regards to perception commonly used detection and discrimination tasks, which are considered to represent the very basics of our perceptual abilities. A detection task requires to identify the presence of a stimulus (e.g. whether a dot is present or absent) whereas a discrimination task (sometimes labelled as categorization) requires to distinguish a target stimulus from other distractors based on certain features (e.g. whether the line is tilted to the left or the right). Although they are both perceptual tasks, evidence has suggested they can lead to differences in objective performance (e.g. [@mack2010jephpp]), which has been investigated extensively in past decades. Until recent years, with increasing popularity in investigating confidence formation, studies showed that detection and discrimination can also lead to distinctive subjective judgements of performance behaviourally ([@meuwese2014app]) and neuroimaging data suggested different neural representations associated with judging confidence in these two tasks (e.g. [@mazor2020e]). Models that have been put forward to explain such differences can be broadly divided into two camps: 1.) Confidence judgements is a higher-order process that treats detection and discrimination distinctively due to the differences in the control of one’s internal state, such as attention; 2.) Behaviour and neural activities differ as a response to the physical features of detection and discrimination at the perceptual level such as amount of sensory evidence and confidence is formed through similar process for these two tasks.

The current project aims to explain the distinctive processes involved in confidence formation in detection and discrimination tasks from the perspective of Signal Detection Theory (SDT), using a new paradigm which includes a discrimination task with the features of detection task. This paradigm also sheds a light on our understanding of alternative models. The following section will first review behavioural and neuroimaging evidence showing differences in detection and discrimination tasks, then discuss some models that have been put forward to explain such differences and finally propose the hypothesis that distinctive patterns arise in forming confidence in the two tasks as a result of their different distributions of signal (target) and noise (distractor) based on SDT.   

## 2.1. Detection vs. Discrimination  
How do we detect and/or categorise something? Does one first detect something is there (e.g. ‘I saw something is there’) and then further categorize it to certain type (e.g. ‘The thing I saw is a car.’), or we know what it is immediately when we see something is there (e.g. ‘I saw a car there)? The level of independence of these two tasks has been the most commonly debated discussion regarding these two tasks ([@mack2010jephpp]; [@grill-spector2005ps]). This is an important consideration for wide range of research themes, as it affects the flexibility of using these two tasks in the investigation of other domains such as confidence formation. Researchers advocate the view that detection and discrimination are two process happen simultaneously 

On the other hand, Mack and Palmeri (2010) used a paradigm to assess the level of dependence of detection and discrimination at the basic-level (car vs. boats) and superordinate level (e.g. car vs. people). They found that detection and basic-level object categorisation is supported by single mechanism as their results revealed a dependence between the success for the detection task and the success for basic-level categorisation task. However, the success for one to correctly categorize a target at superordinate level is not dependent on the success at detection, which suggests distinctive mechanisms supporting detection and superordinate categorisation, thus again highlighting the importance of differentiating the two for future research.  Findings from their research reflects the importance of distinguishing between different levels of stimuli, challenging the view of one underlying perceptual ability for, or at least strong dependence between detection and discrimination suggested previously by Grill-Spector and Kanwisher (2005). 

Detection and discrimination tasks also differ from the perspective that the two alternatives rely on asymmetrical sensory evidence for detection in contrast to symmetrical evidence for discrimination. In a detection task, virtually no sensory evidence is available for participants to make an absence decision, a feature that essentially makes up the definition of ‘absent’ in such tasks, in contrast to a present decision. On the other hand, in discrimination tasks sensory evidence is available for both alternative choices. This results in significant differences from the perspective of SDT (see section 2.5 for details).

_However_, _despite the lack of sensory evidence in absence conditions_, _it has been found that neurons encode both stimulus absence and presence when dissociated from motor response (Merten & Nieder, 2014)_. _In their study_, _they found that prefrontal cortex (PFC) is recruited in rhesus monkeys during stimulus present trials as well as absence trials using single neuron recordings_.

## 2.2. Forming confidence in detection and discrimination 
The ability to have insight into the objective correctness of a response is known as one’s metacognitive sensitivity/accuracy ([@fleming_relating_2010]). Metacognitive sensitivity is measured by calculating the level of correspondence between the Type I objective performance and Type II subjective report. Type II report requires participants to give confidence ratings as participants are instructed to give high confidence ratings when they believed their choice was correct and low confidence ratings if they are less certain. However, it might come as a surprise that our subjective judgements might not always reflect or can even contradict our objective performance ([@kanai2010cc]).  

The study by Kanai and Walsh (2010) was one of the first to show that failure of visual perception is not always picked up by one’s subjective awareness. They used a detection task in which participants were asked to report the presence of the target stimuli. The stimuli were manipulated in six different ways to create six different conditions: contrast, backward masking, flash suppression, dual task, attentional blink and spatial uncertainty, which can be further categorized into attentional difficulty (dual task, attentional blink and spatial uncertainty) and sensory difficulty (contrast, backward masking and flash suppression). Participants then reported their confidence ratings of the correctness of their response after each trial. Beyond using the traditional Type II AUC performance as shown in Fig.1A, they developed a new measure termed Subjective Discriminability of Invisibility (SDI) to measure how accurate participants can adjust their ratings accordingly to their task performance. SDI was developed based on computing the Type II performance but only for misses and correct rejections, i.e. trials in which participants reports absence of the stimulus (Fig.1B). To compare the metacognitive sensitivity in six different conditions, the objective performance of participants was controlled around the level of 70% correct in analysis to avoid situations such as constantly high or low confidence ratings because the task is easy or difficult. The level of 70% correct response was chosen to ensure participants stayed motivated in these trials and they also made enough errors to calculate reliable SDI scores at the same time.

They found that SDI is significantly higher in attentional difficulty conditions compared to sensory difficulty conditions. In other words, participants can accurately judge whether their choice is a correct rejection or a miss when the difficulty arise as a result of increasing attentional demand whereas participants cannot distinguish a miss from correct rejection when the difficulty arise as a result of lacking sensory input. Therefore, the source of noise in perceptual tasks is an important consideration for observers to make confidence judgements; confidence is often adjusted accordingly when noises arises from one’s internal cognitive capacity such as lack of attention whereas the impact of the physical property/environment of perceptual stimuli is less recognised by observer.

More recently, Meuwese et al. (2014) found that metacognitive ability is higher in categorization/ discrimination task than in the detection task for both masked and degraded stimulus. All participants performed the detection and discrimination task, which require them to identify the presence of an animal (e.g. Was there an animal present?) or identify the category (e.g. Was the animal a cat?). The stimuli were either masked by textured patterns or degraded by means of phase scrambling. Participants were then asked to rate how confident they were about the correctness of the choice they made on a scale from 1 to 6. The objective performance in detection and discrimination were also matched at the level of 71% correct to control for the potential effect of objective performance on confidence judgement. Two measures were included for metacognition. One of them is the classic measure of metacognitive sensitivity, the area under the ROC curve. This measure reflects the consistency between subjective confidence ratings of responses made and the actual, objective performance. In addition, they also included the measure of Subjective Discriminability of Visibility (SDV), which includes only trials of hits and false alarms (i.e. trials in which participants reported a stimulus was present), and corresponding Subjective Discriminability of Invisibility (SDI),  which includes only trials of miss and correct rejections (i.e. trials in which participants reported a stimulus was absent).

They found that metacognitive sensitivity, according to the classic measure, is higher for discrimination than detection task (Fig.3). Participants can more accurately evaluate the correctness of their choice when the judgement is about which category the target belongs to than when the it is about whether a target was present. Further analysis revealed that such metacognitive superiority does not exist across whole discrimination task and was in fact, specific for correct rejections and misses, measured by SDI. To further unpack the elements driving the differences between detection and discrimination, the AUC for hits, misses, correct rejections, and false alarms were calculated separately. They found that lower metacognitive accuracy in detection compared with discrimination is driven by lower metacognitive accuracy for correct rejections, i.e. participants subjective judgements of their choice being correct is less accurate for an absence decision when it is physically not there compared with a choice of the stimuli not belonging to the target group when it actually doesn’t. Their results revealed several interesting aspects of confidence formations in these two tasks; people tend to have worse insight for detection than discrimination task, but only in situations where they have correctly rejected the target. 

Consistent with findings in Meuwese et al. (2014), [@arielezylberberg2012fin] found that observers were only influenced for sensory evidence in favour of the selection they made when forming confidence judgements, while the sensory evidence for the unselected choice is irrelevant for such judgements. Although this is only investigated in two discrimination tasks, luminance comparison and random dot motion task, it might provide an explanation for the poor metacognitive performance in detection task. Consider the situation where participants have made a correct “absence” decision, their confidence rating would be in theory be strongly influenced by evidence in favour of this choice according to the findings from Zylberberg, but the lack of sensory evidence in such cases might underlie a disturbance to a more conventional confidence formation process that is engaged in present trials or discrimination trials. Although it is unclear what the consequences of such disturbance are, e.g. does this give rises to the employment of alternative processes such as counter-factual reasoning (discussed in section 2.4 below) specific for absent trials, the uniqueness of confidence formation in absent trials in detection is clear. 

Therefore, given the finding that participants were strongly influenced by the evidence for selected choice, in detection, virtually no sensory evidence is available in absence trials to support confidence judgements, posting a disadvantage for metacognitive accuracy. Broadly, these studies have shown that it is easier to be confident when something is simply not part of the target group than when something is not there at all. 

## 2.3. Computational models of confidence in decision making
The interest in studying confidence formation have extended to developments of computational models. It is widely accepted that human confidence report reflects a Bayesian probabilistic estimation of a choice being correct, which had not been tested rigorously by previously until [@adler2018pcb]. Alder and Ma (2018) tested several models for forming confidence including the Bayesian model and Non-Bayesian alternatives using two categorisation tasks.  The tasks essentially require the participants to report whether the stimulus shown is a gabor or ellipse, as well as the level of confidence in their choices simultaneously in one press. The contrast of the gabor and elongation of the ellipse were manipulated to give rise to the measure of stimuli reliability and corresponds to the level of sensory uncertainty.  Their study included two variations of the task such that in task A the two types of stimuli are drawn from Gaussian distributions with different means but same SD, whereas in task B the stimuli are drawn from Gaussian distributions with different SD but same means. The consequence of such manipulation is that in task A, stimuli around tilted to certain direction are more likely to be from gabor category, whereas in task B, stimuli around the horizontal are more likely to be from the gabor category. 
An ideal Bayesian observer would make decision by computing a posterior probability of choosing category c given measurement _x_, _p(c|x)_, which is equivalent to the positive log posterior ratio _d_ = $\frac{\textit{p(c=1|x)} }{\textit{p(c=1|x)} }$. The differences in the structure of task A and B have an impact on how Bayesian observers make decisions based on computing this posterior probability such that in task A, participants report stimuli belongs to category 1 when measurement x is positive whereas in task B participants needs both x and trial-by-trial sensory uncertainty to make optimal decisions. 
They found that participants’ confidence reports do not reflect a probability of a choice being correct in a completely Bayesian way but rather a heuristic approximation of this choice being correct. Subjects however do take into account of sensory uncertainty, the perquisite of Bayesian reasoning. From model fit results, it was suggested that participants use the knowledge of their sensory uncertainty over a category, without computing a Bayesian posterior distribution described above but rather subjects base their response on a maximum posteriori estimate of orientation (using the mixture of the two stimulus distributions as a prior distribution). Models that do not include sensory uncertainty, e.g. fixed models provide a poor fit for the data. Bayesian model does not fully account for the behavioural results and among all models, heuristic models incorporating uncertainty in a non-Bayesian way is a better fit for their results, which suggests that observers do take into account of uncertainty when forming confidence judgements but in a non-Bayesian manner.


## 2.4. Neural basis of forming confidence 
What brain regions support the formation of metacognitive judgements? Studies have shown a rostrolateral and dorsolateral prefrontal cortices (PFCs) are key brain areas for metacognition ([@fleming_relating_2010], see [@fleming2012ptrsb] for review) in human. Miyamato et al. (2018) found that area 10 is also recruited for metacognitive evaluation of non-experienced events in macaque monkeys and the frontopolar cortex plays a causal role to confer awareness of non-experienced events.

Given the behavioural studies described above showing different cconfidence judgement formation in discrimination and detection, it is reasonable to expect distinctive neural representations supporting these judgements. [@mazor2020e] investigated the neural contributions to confidence judgements in detection and discrimination. Participants were asked to judge whether a grating was present (detection) or the orientation of a grating(discrimination), after which they reported their confidence ratings. They found distinct neural representations of confidence in detection and discrimination. Specifically, a more pronounced quadratic effect of confidence was observed in the frontopolar cortex (FPC) in the detection task compared with the discrimination task. In their study, they proposed two different approaches to explain this pattern which will be discussed below.  

## 2.5. Counterfactual-reasoning model for confidence formation
One explanation for the findings from [@mazor2020e] concerns the qualitative differences of detection and discrimination, and more specifically, the unique role of counterfactual reasoning and its interaction with one’s attentional state in detection. Due to the lack of sensory input for absence trials, participants might instead refer to their perceived likelihood of having detected a hypothetical target i.e. the counterfactual situation, in order to make confidence rating for target absence. According to this account, the differences of FPC activations in detection and discrimination might reflect a role of this area in referring to an internal hypothetical state. This process is expected to be influenced by their current state of attention: participants are likely to give low confidence ratings when they evaluate themselves as less attentive as their judgements about the possibility of the counterfactual situation is less certain and vice versa. Behavioural results have supported a role of attention monitoring in subject awareness of performance [@kanai2010cc]. People seem to be able to distinguish their correct rejections from misses when the task difficulty arises from competing attention resources in contrast to when it arises from lack of sensory input. In discrimination task, it is also found that uncertainty related to attention influences observers’ categorization performance and confidence rating in a Bayesian style [@denison2018pnasusa]. Therefore, the process of a counterfactual reasoning is likely in the absence trials for detection.  

However, it is important to note here that such counterfactual model incorporating attentional control could only account for distinct activations of FPC in ‘No’ responses whereas Mazor et al. (2020) found similar pattern in ‘Yes’ trials as well. Further investigations are therefore needed to confirm the role of counterfactual reasoning in detection and discrimination. 

## 2.6. SDT Model for confidence formation
Another explanation based on Signal Detection Theory (SDT) concerns the quantitative differences in these tasks. In a detection task, the sensory evidence for two competing choices is unequally distributed, with a greater variance in the presence trials than the absence trials (Fig.1b) as theoretically no sensory stimulus is available in absence trials. In contrast, in a typical discrimination task the two alternatives are equally distributed (Fig.1a). According to the likelihood-ratio calculation of decision making for a binary task, Log$\frac{(\textit{l(h1)} }{(l(h2)}$ ，equal distribution implies a linear function of log likelihood-ratio calculation in comparison to a quadratic function from an unequally distributed sample [@wickens2002]. Therefore, a more pronounced quadratic effect of confidence in FPC in the detection task might arise as a result ofactivation  in this area representing the relative likelihood of committing to ‘Yes’ or ‘No’ responses drawing from unequally distributed evidence, whereas it is linear when neurons were representing the relative likelihood of ‘Left’ or ‘Right’ drawing from equally distributed evidence. In line with this model, a discrimination task with the attributes of unequally distributed evidence for two alternatives as that in a detection task would also lead to similar quadratic effects of confidence modulation in FPC.


## 2.7. The current study 
To untangle the counterfactual and SDT models described above, the current study includes another discrimination task with unequal distributions (Fig.4C) in addition to a detection and a discrimination task that are typically used in psychophysics. This will allow us to separate the quantitative feature of the task, i.e. type of distribution, from the qualitative ones, i.e. type of task. We propose an experiment paradigm including three perceptual tasks while participants’ brain activations are being monitored in an MR scanner: A). discrimination task with equal variance (whether gratings are tilted to the right or left; B). detection task with unequal variance (whether a grating is present or not); C). discrimination task with unequal variance (whether a stimulus is a vertical grating or tilted towards any direction). Participants are asked to rate their confidence about their decision following each trial. If quadratic effects of confidence on brain activations are observed in both the discrimination task with unequal variance and the detection task, it is likely that neurons in this area represent the relative likelihood of two competing choices in perceptual tasks as predicted by the SDT model. On the other hand, if the discrimination task with unequal variance shares a similar linear activation profile with the equal-variance discrimination task, it would suggest that the difference in neural representations of detection and discrimination is driven by some more qualitative features of detection, such as counterfactual reasoning proposed in current study. 

## 2.8. Aims and Hypothesis
The current study aims to study how human form confidence in detection and discrimination tasks both from behavioural and neural perspectives. Specifically, this project aims to:

*Compare the confidence formation pattern of the new tilt recognition task with detection and discrimination.

Replicate the finding from [@mazor2020e] that an interaction between task (discrimination/detection) and the quadratic effect of confidence, in medial and lateral frontopolar cortex, as well as in the STS and preSMA.

*replicate the finding from [@mazor2020e] of an interaction between detection response (present/absent) and the linear effect of confidence in the right TPJ. 

*Compare the quadratic effect of confidence in the tilt-recognition task with the quadratic effect of confidence in the detection and discrimination tasks in the frontopolar cortex, the STS and the pre-SMA.

*Compare the response-specific linear effects of confidence in the tilt- recognition task with the response-specific linear effects of confidence in the detection and discrimination tasks in the right TPJ.

# 3.Method 
## 3.1. Overview
The current study involved two parts: a behavioural training session lasting around 60 mins and a scanning session lasting around 90 mins with intervals no longer than two weeks. Both parts of the experiments took place at the Wellcome Centre for Human Neuroimaging, University College London. The scanning session was conducted in the 3 Tesla MRI scanner. The ethics of the current study was approved by XX. 

## 3.2. Participants
XX healthy participants were recruited, xx completed the behavioural training session and xx completed the scanning session. According to the preregistration exclusion criteria (see section 3.6. below for details), in total data from 25 participants were included in the final analysis. Participants received cash payments as compensation for their time, £10 for the behavioural session and £20 for the scanning session. To motivate participants to perform their best in our tasks, we also offered bonus payment for good performance and accurate confidence ratings (see procedure below for details on bonus calculation).  
 
## 3.3. Experimental Procedure 
### 3.3.1. Behavioural session
During behavioural training session, participants first received introductions of the study, including general procedure, ethic and data protection protocols. The structure of the three tasks (see fig.4 for the schematic representation) were explained to participants as following:

* Detection task: On half of the trials, a noisy grating will appear after the fixation cross and on the other half there would be no grating shown and you need to decide whether there was a grating present.

* Discrimination task: A grating will appear on the screen every few seconds after the fixation cross, which will be tilted clockwise in half of the trials and anticlockwise in other half. Participants were asked to decide which direction the grating was tilted to. 

* Tilt recognition task: A grating will appear on the screen every few seconds after the fixation cross, which will be tilted (to any direction) in half of the trials and vertical in other half. Participants were asked to decide whether the grating was tilted or vertical. 

* Confidence rating: In all three tasks, immediately after making a choice, you need to indicate how confident you are in your decision by changing the size of the circle.

This session contains a practice block, a calibration block and several training blocks for all three tasks. The response mapping will be counterbalanced between blocks, such that an index finger press will be used to indicate a clockwise tilt on half of the trials, and an anticlockwise tilt on the other half. Similarly, in half of the tilt recognition trials the index finger will be mapped to a vertical response, and on the other half to a tilted response. Lastly, in half of the detection trials the index finger will be mapped to a yes (‘target present’) response, and on the other half to a no (‘target absent’) response. To avoid size-related effect on confidence rating, participants were divided into two groups such that for half participants bigger circle corresponds to higher confidence level and for the other half smaller circle corresponds to higher confidence level.

During this session, each participants performance was controlled around 70 % accurate, by manipulating the task difficulty independently for the three tasks. This will be achieved by using the common 1 up 2 down staircase procedure on stimulus visibility (discrimination and detection task) and on the standard deviation of the orientation distribution (tilt recognition). Participants were not invited back to continue the scanning session if: 1.) their accuracy were lower than 60% or higher than 80%; 2.) had strong response bias, i.e. used the same response in more than 80% of the trials; 3.) had strong confidence bias, i.e. the same confidence level was reported for more than 90% of the trials. 

### 3.3.2. Scanning session
The structure of the three tasks were the same as behavioural session. To motivate participants perform we offered bonus in addition to the baseline payment for the scanning session.  Bonus is calculated use following rule:		
bonus=£$\frac{\overrightarrow{accuracy}.\overrightarrow{confidence}}{200}$. Where $\overrightarrow{accuracy}$ is a vector of 1 and -1 for correct and incorrect responses, and $\overrightarrow{confidence}$ is a vector of integers in the range of 1 to 6, representing confidence reports for all trials. The rule for bonus calculation was explained to participants in both sessions. The scanning session started with a calibration phase to further calibrate participants performance during which time the structural scan for each participant was also obtained. At scanning, 10 discrimination and detection blocks were presented in 5 scanner runs. 

## 3.4. Stimulus
After a temporal rest period of 500-4000 milliseconds, each trial will start with a fixation cross (500 milliseconds). The target was then presented on the screen for 500 milliseconds. In all three conditions, stimuli will consist of 10 grayscale frames presented at 20 frames per second within a circle of diameter 3°. Stimuli will be generated in the following way:

* Generate 10 grayscale frames ( _F_ ... _F_ ), each an array of 142 by 142 random luminance values.
* Create a 142 by 142 sinusudial grating ( 24 pixels per period, random phase). The orientation of the grating is determined according to the trial type.
* The grating visibility for frame _i_ is _pi_ = _v_ × _exp_(-$|\textit{i}-5|$/2) with _v_ being the visibility level in this trial (0 for target-absent trials).
* For each pixel in the frame , replace the luminance value for this pixel with the luminance value of this pixel in the grating with a probability of.

## 3.5. Scanning parameters 
Scanning took place at the Wellcome Centre for Human Neuroimaging, London. The structural images were obtained using an MPRAGE sequence (1x1x1 _mm_ voxels, 176 slices, in plane FoV = 256x256 _mm_ 2), followed by a double-echo FLASH (gradient echo) sequence with TE1=10ms and TE2=12.46ms (64 slices, slice thickness = 2 _mm_, gap = 1 _mm_, in plane FoV= 192×192 _mm_ 2, resolution = 3×3 _mm_ 2) that were later used for field inhomogeneity correction. Functional scans were acquired using a 2D EPI sequence, optimized for regions near the orbitofrontal cortex (3.0x3.0x3.0 _mm_ voxels, TR=3.36 seconds, TE = 30 ms, 48 slices tilted by -30 degrees with respect to the T¿C axis, matrix size = 64x72, Z-shim=-1.4).

```{r include=FALSE, cache=TRUE}

#open file
Data <- read.csv("Data.csv")

#creat a new file with only includedtrials 
Data_Valid <- Data %>% 
  #Filter out the excluded trials 
  filter(inclusion==1) %>%
  filter(! response=="NaN") %>%
  mutate(rt=response_time*1000) %>%
  mutate(log_rt=log(rt))

# MM: since you're not using Behavioural subj anywhere I figured 
# this was a more economical way to get the number of subjects:
no_subj <- length(unique(Data_Valid$subj_id))

Data_Det <- Data_Valid %>% 
  #only include detection 
  filter(task=="Detection")

Data_Dis <- Data_Valid %>% 
  #only include discrimination
  filter(task=="Discrimination")

Data_Til<- Data_Valid %>% 
  #only include tilt recognition 
  filter(task=="Tilt")

#count the total no of trials for each task for each participant
total_trials <- Data_Valid %>%
  group_by(subj_id, task) %>%
  summarise(totaltrials=(count=n()))

#get the number of trials for each participants where the stimulus shown is 1 (i.e.Present, Clockwise & Tilted)
sti_trials <- Data_Valid %>%
  group_by(subj_id, task) %>%
  filter(stimulus=="1") %>%
  summarise(sti_trials=(count=n()))
sti_trials

#count no. of correct trials for each task for each participant
individual_trials <- Data_Valid %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id, task, accuracy) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  #combine with the total data frame
  inner_join(total_trials, by = c ("subj_id","task")) %>%
  mutate(proportion = no_trials / totaltrials)
individual_trials

#display accuracy 
individual_accuracy <- individual_trials %>%
  filter(accuracy==1) %>%
  rename(correct=proportion)
individual_accuracy
```

# 4.Results
## 4.1.Performance across threee tasks
```{r, include=FALSE, cache=TRUE}
#display accuracy for each task 
task_accuracy <- individual_accuracy %>%
  group_by(task) %>%
  summarise(mean(correct)) %>%
  spread(key='task', value='mean(correct)')
task_accuracy

#ANOVA comparing task performance 
aov.Mean_accuracy <- aov(correct~task,data=individual_accuracy)

## MM: you can then use apa_print(aov.Mean_accuracy) to present stats.

#############################
#calculating HIT and FA rate
#############################

#combine with the ptrials data frame
trials <- Data_Valid %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id, task, accuracy, response) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  inner_join(sti_trials, by = c ("subj_id","task"))
trials

# MM: no need to assert variable names after defining them
# trials

#calculating HIT FA rate for detection
hit_det <- trials %>% 
  filter(task=="Detection", response== "1", accuracy=="1") %>%
  mutate(hit_rate = no_trials / sti_trials) %>%
  mutate(z_hit= qnorm(hit_rate))
hit_det

#calculating mean HIT rate for detection
mean_mean_det_hit <- mean(hit_det$hit_rate)

fa_det <- trials %>% 
  filter(task=="Detection", response== "1", accuracy=="0") %>%
  mutate(fa_rate = no_trials / sti_trials) %>%
  mutate(z_fa= qnorm(fa_rate)) %>%
  inner_join(hit_det, by = c ("subj_id","task", "response", "sti_trials")) %>%
  mutate(dprime= z_hit-z_fa)
fa_det

#calculating mean FA rate for detection
det_fa <- mean(fa_det$fa_rate)

#calculating HIT FA for discrimination
hit_dis <- trials %>% 
  filter(task=="Discrimination", response== "1", accuracy=="1") %>%
  mutate(hit_rate = no_trials / sti_trials) %>%
  mutate(z_hit= qnorm(hit_rate))
hit_dis

fa_dis<- trials %>% 
  filter(task=="Discrimination", response== "1", accuracy=="0") %>%
  mutate(fa_rate = no_trials / sti_trials)%>%
  mutate(z_fa= qnorm(fa_rate)) %>%
  inner_join(hit_dis, by = c ("subj_id","task", "response", "sti_trials")) %>%
  mutate(dprime= z_hit-z_fa)
fa_dis

#calculating HIT FA rate for tilt
hit_til <- trials %>% 
  filter(task=="Tilt", response== "1", accuracy=="1") %>%
  mutate(hit_rate = no_trials / sti_trials) %>%
  mutate(z_hit= qnorm(hit_rate))
hit_til

til_hit <- mean(hit_til$hit_rate)

fa_til<- trials %>% 
  filter(task=="Tilt", response== "1", accuracy=="0") %>%
  mutate(fa_rate = no_trials / sti_trials) %>%
  mutate(z_fa= qnorm(fa_rate)) %>%
  inner_join(hit_til, by = c ("subj_id","task", "response", "sti_trials")) %>%
  mutate(dprime= z_hit-z_fa) 
fa_til
til_fa <- mean(fa_til$fa_rate)

#merge together 
d_prime <- rbind(fa_det, fa_dis, fa_til)

#d values for each task
d_det <- mean(fa_det$dprime)
d_dis <- mean(fa_dis$dprime)
d_til <- mean(fa_til$dprime)

# MM: I want to encourage you to use functions more where you write the same
# script over and over again. Having code duplicates in your code is reallllly
# error-prone. So whenever you find yourself having to write the same thing 
# more than once (e.g. you extract d' for each task in a very similar way), 
# write a function instead. I would encourage you to write a function 'get_d'
# that takes a df as input and returns the d' for this table, and then groupby task 
# and subject and use summarise with your new function.


#ANOVA Calculation for d prime
aov.d <- aov(dprime~task, data=d_prime)

# MM: here also, no need to extract ps and fs, you can use apa_print() instead :)
summary(aov.d)
p2 <- summary(aov.d)[[1]][["Pr(>F)"]][[1]]
F2<- summary(aov.d)[[1]][["F value"]][[1]]

###########################
#response bias calculation
###########################
response <- Data_Valid %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id,task, response) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  #combine with the total data frame
  inner_join(total_trials, by = c ("task","subj_id")) %>%
  mutate(probability = no_trials / totaltrials)
response

#response bias for detection
det_resp <- response %>%
  filter(task=="Detection") %>%
  filter(response=="1")
det_resp

# MM: Here's a really cool thing: instead of extracting mean and sd, 
# you can use apa_print(t.test(...))$estimate instead!

det_bias <- mean(det_resp$probability)
sd_bias1 <- sd(det_resp$probability)

# MM: what is this variable?
detection<- (rnorm(no_subj, mean = det_bias, sd = sd_bias1))
# MM: aren't you saving the t.test results to a variable?
t_det <- t.test(detection, mu = 0.5) # Ho: mu = 0.5

# MM: same principle here. if you are writing the same code three times,
# it's better to write a function instead :)

#response bias for discrimination
dis_resp <- response %>%
  filter(task=="Discrimination") %>%
  filter(response=="1")
dis_resp

dis_bias <- mean(dis_resp$probability)
sd_bias2 <- sd(dis_resp$probability)
discrimination<- (rnorm(no_subj, mean = dis_bias, sd = sd_bias2))
t.test(discrimination, mu = 0.5) # Ho: mu = 0.5

#response bias for tilt recognition
til_resp <- response %>%
  filter(task=="Tilt") %>%
  filter(response=="1")
til_resp

til_bias <- mean(til_resp$probability)
sd_bias3 <- sd(til_resp$probability)
tilt<- (rnorm(no_subj, mean = til_bias, sd = sd_bias3))
t.test(tilt, mu = 0.5) # Ho: mu = 0.5

#####################
#calculating rt
#####################
rt <- Data_Valid %>%
  group_by(subj_id, task, response, accuracy) %>%
  summarise(median(rt)) %>%
  rename("median_rt"="median(rt)")
rt

#calculating median rt for correct and incorrect trials
rt_accuracy <- rt %>%
  group_by(accuracy) %>%
  summarise(median(median_rt)) %>%
  rename("median_rt"="median(median_rt)") 
rt_accuracy

rt_accuracy$accuracy[rt_accuracy$accuracy=="0"]<-"incorrect"
rt_accuracy$accuracy[rt_accuracy$accuracy=="1"]<-"correct"

#calling out median rt for correct and incorrect trials
getmedian <- rt_accuracy$'median_rt'
names(getmedian) <-rt_accuracy$accuracy

#calling out quantiles of rt for correct and incorrect trials
correct_trials <- rt %>%
  filter(accuracy==1)
correct_trials

incorrect_trials <- rt %>%
  filter(accuracy==0)
incorrect_trials 

# MM: why not use qt_correct <- quantile(correct_trials$median_rt, c(.25,.5,.75)) instead?
qt1_correct <- quantile(correct_trials$median_rt, c(.25)) 
qt3_correct <- quantile(correct_trials$median_rt, c(.75))
qt1_incorrect <- quantile(incorrect_trials$median_rt, c(.25)) 
qt3_incorrect <- quantile(incorrect_trials$median_rt, c(.75))

#get t test results detection, discrimination and tilt 
# MM: I would use papaja instead here.
tdet <-t.test(log_rt~response, mu=0, alt="two.sided", conf=0.95,var.equal=TRUE, data=Data_Det)$statistic
p.det <- t.test(log_rt~response, mu=0, alt="two.sided",conf=0.95,var.equal=TRUE, data=Data_Det)$p.value

tdis <-t.test(log_rt~response, mu=0, alt="two.sided", conf=0.95,var.equal=TRUE,data=Data_Dis)$statistic
p.dis <-t.test(log_rt~response, mu=0, alt="two.sided", conf=0.95,var.equal=TRUE,data=Data_Dis)$p.value

ttil <-t.test(log_rt~response, mu=0, alt="two.sided", conf=0.95,var.equal=TRUE,data=Data_Til)$statistic
p.til <-t.test(log_rt~response, mu=0, alt="two.sided",conf=0.95,var.equal=TRUE,data=Data_Til)$p.value

```

Detection (accuracy= `r printnum(task_accuracy$Detection)`, d'= `r apa(d_det, 2, T)`),  discrimination (accuracy= `r printnum(task_accuracy$Discrimination)`, d'= `r apa(d_dis, 2, T)`) and tilt recognition (accuracy = `r printnum(task_accuracy$Tilt)`, d'= `r apa(d_til, 2, T)`) was similar. A one-way ANOVA failed to detect a significant difference between the accuracy of these three tasks `r apa_print(aov.Mean_accuracy)$full_result$task` and d' (F= `r apa(F2, 2, T)`, p= `r apa(p2, 2, T)`; see Figure \ref{fig:accuracy})).

The probability of responding Yes in detection was `r apa(print(det_bias), 2, T)` (± `r apa(sd_bias1, 2, T)`), and was significantly different from 0.5 (ADD T TEST RESULTS). The probability of responding CLOCKWISE was `r apa(dis_bias, 2, T)` (± `r apa(sd_bias2, 2, T)`) and was not significantly different from 0.5. For the tilt recognition task, the probability of responding TILTED was (`r apa(til_bias, 2, T)` ± `r apa(sd_bias3, 2, T)`).

Response time was faster for correct response (1st quartile= `r apa(qt1_correct, 2, T)`, median= `r apa(unname(getmedian['correct']), 2, T)`, 3rd quartile= `r apa(qt3_correct, 2, T)` milliseconds) than incorrect responses (1st quartile= `r apa(qt1_incorrect, 2, T)`, median= `r apa(unname(getmedian['incorrect']), 2, T)`, 3rd quartile= `r apa(qt3_incorrect, 2, T)` milliseconds). A one-way analysis of variance failed to detect a significant overall effect of responses type in detection (Yes vs. NO, t=`r apa(tdet, 2, T)` p=`r apa(p.det, 2, T)` ), discrimination (CLOCKWISE vs. ANTICLOCKWISE, t=`r apa(tdis, 2, T)`, p= `r apa(p.dis, 2, T)`) and tilt recognition (VERTICAL vs. TILTED, t=`r apa(ttil, 2, T)`, p= `r apa(p.til, 2, T)`) on response time.

```{r, accuracy_figure, echo=FALSE, message=FALSE,fig.cap="\\label{fig:accuracy} Mean accuracy across three tasks", cache=TRUE}
## MM: note how I've included fig.cap in the header, to include a caption and also a label (fig:accuracy) that we can later reference in the text.
number_ticks <- function(n) {function(limits) pretty(limits, n)}
data <- individual_accuracy
ggplot(data, mapping = aes(x= task, y=correct)) + ylim(0.4, 1) +labs (y="mean accuracy")+ theme_classic() +
   geom_boxplot()+ geom_jitter(position=position_jitter(width=.1, height=0), alpha=0.3, size=4) + scale_y_continuous(breaks=number_ticks(6), limits=c(0.4, 1)) + geom_hline(yintercept=0.5, linetype="dashed", color = "black") + theme(plot.title = (element_text(color = "black", size = 12, face = "italic")))
```

```{r include=FALSE, cache=TRUE}
#mean confidence frequency 
# MM: instead of creating three different dfs, I kept everything in one df here:
confidence_distribution <- Data_Valid %>%
  group_by(task,confidence,response) %>%
  summarise(no_trials=(count=n())) %>%
  mutate(frequency = no_trials / no_subj)
confidence_distribution

confidence_distribution$response[(confidence_distribution$response=="0") &
                                   confidence_distribution$task=='Detection'] <- "No"

confidence_distribution$response[(confidence_distribution$response=="1") &
                                   confidence_distribution$task=='Detection'] <- "Yes"

confidence_distribution$response[(confidence_distribution$response=="0") &
                                   confidence_distribution$task=='Discrimination'] <- "Anticlockwise"

confidence_distribution$response[(confidence_distribution$response=="1") &
                                   confidence_distribution$task=='Discrimination'] <- "Clockwise"

confidence_distribution$response[(confidence_distribution$response=="0") &
                                   confidence_distribution$task=='Tilt'] <- "Vertical"

confidence_distribution$response[(confidence_distribution$response=="1") &
                                   confidence_distribution$task=='Tilt'] <- "Tilted"

confidence_distribution$response <- factor(confidence_distribution$response, levels=c('No','Yes','Anticlockwise','Clockwise','Vertical','Tilted'))

det <- Data_Valid %>%
  filter(task=="Detection") 
det

dis <- Data_Valid %>%
  filter(task=="Discrimination") 
dis

til <- Data_Valid %>%
  filter(task=="Tilt") 
til

p1 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = det)$p.value
t1 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = det)$statistic
p3 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = til)$p.value
t3 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = til)$statistic
```
&nbsp;

## 4.2.Confidence distributions
Within detection, a significant difference in mean confidence was observed between Yes (target present) and NO (target absent) responses (see Fig.4 above) (t=`r apa(t1, 2, T)`, p = `r myround_p (apa(p1, 2, T))`) , such that participants are more confident in their Yes responses than NO response and a statistical significance was also observed in the tilt recognition task between TILTED and Vertical response (t=`r apa(t3, 2, T)`, p = `r myround_p(apa(p3, 2, T))`; see Figure \ref{fig:confidence}).

```{r echo=FALSE, message=FALSE, fig.cap="\\label{fig:confidence} Confidence distributions across three tasks and responses. Error bars represent the standard error of the mean.", cache=TRUE}

#SE calculation  (the standard deviation divided by the square root of the sample size)
#se <- function(x) sqrt(var(x)/length(x))
pd <- position_dodge(0.5) # move them .05 to the left and right
ci <- se(confidence_distribution$frequency)
         
#plot the frequency of confidence ratings for all three tasks
task_labs = c('Detection','Discrimination','Tilt Recognition')
names(task_labs) = c('Detection','Discrimination','Tilt')
cbPalette <- c("#e41a1c", "#377eb8","#4daf4a", "#984ea3","#999999","#f781bf")

confidence_det <- ggplot(confidence_distribution, mapping = aes(x = confidence, y= frequency, fill= response)) +
  geom_bar(stat="identity", width = 0.65, position = position_dodge(width = 0.7))+ scale_fill_manual(values=cbPalette) + theme_classic() + labs(title="Confidence by task and response", y="frequency")+ scale_x_continuous(breaks=number_ticks(6)) +ylim(0,30)+
  theme(plot.title = (element_text(color = "black", size = 12, face = "italic")))
confidence_det+facet_grid(rows=vars(task),
                          labeller = labeller(task=task_labs))
```

&nbsp;

&nbsp;

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=3, cache=TRUE}
#Type 2 ROC preparation
sti <- Data_Valid %>%
  group_by(subj_id, task, stimulus) %>%
  summarise(total_sti_trials=(count=n())) 
sti

#get the total number of trials for correct and incoccrect trials
acc <- Data_Valid %>%
  group_by(subj_id, task, accuracy, response) %>%
  summarise(total_acc_trials=(count=n())) 
acc

conf_prob_acc <- Data_Valid %>%
  group_by(subj_id, task, stimulus, response, accuracy, confidence) %>%
  summarise(no_trials=(count=n())) %>%
  inner_join(acc, by = c ("subj_id","task", "response","accuracy")) %>%
  mutate(prob_acc= no_trials/total_acc_trials)
conf_prob_acc

conf_prob <- Data_Valid %>%
  group_by(subj_id, task, stimulus, response, accuracy, confidence) %>%
  summarise(no_trials=(count=n())) %>%
  inner_join(sti, by = c ("subj_id","task", "stimulus")) %>%
  mutate(prob= no_trials/total_sti_trials) %>%
  inner_join(conf_prob_acc, by = c ("subj_id","task", "stimulus", "response", "confidence","accuracy", "no_trials"))
conf_prob

###############
#  Detection  #
###############
#Detection peparation
#include only trials for detection
conf_det <- conf_prob %>%
  filter(task=="Detection")
conf_det

#get incorrect trials 
conf_det0 <- subset(conf_det, accuracy=="0", select=c(confidence, response, prob_acc))
conf_det0 <- conf_det0[order(conf_det0[,1,2]),]
sex1 <- se(conf_det0$prob_acc)


# MM: in the following lines, why use many dfs instead of piping the commands?
#get incorrect trials with 1 response
confdet_incorrect1 <- conf_det0 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="1")
confdet_incorrect1

#get incorrect trials with 0 response
confdet_incorrect0 <- conf_det0 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="0")
confdet_incorrect0

#order df with descending confidence
confdet_incorrect0 <- confdet_incorrect0[order(-confdet_incorrect0$confidence),]
confdet_incorrect1 <- confdet_incorrect1[order(-confdet_incorrect1$confidence),]

#get cumulative prob
confdet_incorrect0[,'prob0']=cumsum(confdet_incorrect0[,'prob'])
confdet_incorrect1[,'prob0']=cumsum(confdet_incorrect1[,'prob'])

#bind incorrect trials together
confdet_incorrect <- rbind(confdet_incorrect0, confdet_incorrect1)

#get correct trials
conf_det1 <- subset(conf_det, accuracy=="1", select=c(confidence, response, prob_acc))
conf_det1 <- conf_det1[order(conf_det1[,1,2]),]
sey1 <- se(conf_det1$prob_acc)

#get correct trials with 1 response
confdet_correct1 <- conf_det1 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="1")
confdet_correct1 

#get correct trials with 0 response
confdet_correct0 <- conf_det1 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="0")
confdet_correct0

#order df with descending confidence
confdet_correct0 <- confdet_correct0[order(-confdet_correct0$confidence),]
confdet_correct1 <- confdet_correct1[order(-confdet_correct1$confidence),]

#get cumulative prob
confdet_correct0[,'prob1']=cumsum(confdet_correct0[,'prob'])
confdet_correct1[,'prob1']=cumsum(confdet_correct1[,'prob'])

#bind correct trials together
confdet_correct <- rbind(confdet_correct0, confdet_correct1)

#bind incorrect and correct trials together
confdet_acc <- inner_join(confdet_incorrect, confdet_correct, by=c("confidence", "response"))

#add the row for zero point for the ROC plot
confdet_acc <- ungroup(confdet_acc)

confdet_acc <- confdet_acc %>% add_row(confidence=7, response=0, prob_acc.x=0, prob.x=0, prob0=0, prob_acc.y=0, prob.y=0, prob1=0) %>% add_row(confidence=7, response=1, prob_acc.x=0, prob.x=0, prob0=0, prob_acc.y=0, prob.y=0, prob1=0)

#change response code
confdet_acc$response[confdet_acc$response=="0"] <- "No"
confdet_acc$response[confdet_acc$response=="1"] <- "Yes"

#calculating area under the curve
auc_detection <- t(sapply(split(confdet_acc, confdet_acc$response), function(x) {
  data.frame(response=x$response[1], auc=auc(x$prob0, x$prob1, type='spline'))
}))
AUC_det <- merge(confdet_acc, auc_detection)
AUC_det$auc <- as.numeric(AUC_det$auc)
auc_det <- AUC_det$'auc'
names(auc_det) <-AUC_det$response

#t test comparing area under the curve 
#det_auc<- (rnorm(no_subj, mean = unname(auc_det['No']), sd = sd_bias2))
#t.test(det_auc, mu = unname(auc_det['Yes'])) # 

#det_auc <- (rnorm(no_subj, mean = dis_bias, sd = sd_bias2))
#t.test(det_auc, mu = unname) # Ho: mu = 0.5

####################
#  Discrimination  #
####################
conf_dis <- conf_prob %>%
  filter(task=="Discrimination")
conf_dis

#get incorrect trials 
conf_dis0 <- subset(conf_dis, accuracy=="0", select=c(confidence, response, prob_acc))
conf_dis0 <- conf_dis0[order(conf_dis0[,1,2]),]

confdis_incorrect1 <- conf_dis0 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="1")
confdis_incorrect1

confdis_incorrect0 <- conf_dis0 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="0")
confdis_incorrect0

confdis_incorrect0 <- confdis_incorrect0[order(-confdis_incorrect0$confidence),]
confdis_incorrect1 <- confdis_incorrect1[order(-confdis_incorrect1$confidence),]

confdis_incorrect0[,'prob0']=cumsum(confdis_incorrect0[,'prob'])
confdis_incorrect1[,'prob0']=cumsum(confdis_incorrect1[,'prob'])

confdis_incorrect <- rbind(confdis_incorrect0, confdis_incorrect1)

conf_dis1 <- subset(conf_dis, accuracy=="1", select=c(confidence, response, prob_acc))
conf_dis1 <- conf_dis1[order(conf_dis1[,1,2]),]

confdis_correct1 <- conf_dis1 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="1")
confdis_correct1

confdis_correct0 <- conf_dis1 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="0")
confdis_correct0

confdis_correct0 <- confdis_correct0[order(-confdis_correct0$confidence),]
confdis_correct1 <- confdis_correct1[order(-confdis_correct1$confidence),]

confdis_correct0[,'prob1']=cumsum(confdis_correct0[,'prob'])
confdis_correct1[,'prob1']=cumsum(confdis_correct1[,'prob'])

confdis_correct <- rbind(confdis_correct0, confdis_correct1)
confdis_acc <- inner_join(confdis_incorrect, confdis_correct, by=c("confidence", "response"))
confdis_acc <- ungroup(confdis_acc)
confdis_acc <- confdis_acc %>% add_row(confidence=7, response=0, prob_acc.x=0, prob.x=0, prob0=0, prob_acc.y=0, prob.y=0, prob1=0) %>% add_row(confidence=7, response=1, prob_acc.x=0, prob.x=0, prob0=0, prob_acc.y=0, prob.y=0, prob1=0)
confdis_acc$response[confdis_acc$response=="0"] <- "Anticlockwise"
confdis_acc$response[confdis_acc$response=="1"] <- "Clockwise"

#Tilt recognition
conf_til <- conf_prob %>%
  filter(task=="Tilt")
conf_til

conf_til0 <- subset(conf_til, accuracy=="0", select=c(confidence, response, prob_acc))
conf_til0 <- conf_til0[order(conf_til0[,1,2]),]

conftil_incorrect1 <- conf_til0 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="1")
conftil_incorrect1

conftil_incorrect0 <- conf_til0 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="0")
conftil_incorrect0

conftil_incorrect0 <- conftil_incorrect0[order(-conftil_incorrect0$confidence),]
conftil_incorrect1 <- conftil_incorrect1[order(-conftil_incorrect1$confidence),]

conftil_incorrect0[,'prob0']=cumsum(conftil_incorrect0[,'prob'])
conftil_incorrect1[,'prob0']=cumsum(conftil_incorrect1[,'prob'])

conftil_incorrect <- rbind(conftil_incorrect0, conftil_incorrect1)

conf_til1 <- subset(conf_til, accuracy=="1", select=c(confidence, response, prob_acc))
conf_til1 <- conf_til1[order(conf_til1[,1,2]),]

conftil_correct1 <- conf_til1 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="1")
conftil_correct1

conftil_correct0 <- conf_til1 %>%
  group_by(confidence, response) %>%
  summarise_each(funs(sum)) %>%
  mutate(prob= prob_acc/no_subj) %>%
  filter(response=="0")
conftil_correct0

conftil_correct0 <- conftil_correct0[order(-conftil_correct0$confidence),]
conftil_correct1 <- conftil_correct1[order(-conftil_correct1$confidence),]

conftil_correct0[,'prob1']=cumsum(conftil_correct0[,'prob'])
conftil_correct1[,'prob1']=cumsum(conftil_correct1[,'prob'])
conftil_correct <- rbind(conftil_correct0, conftil_correct1)
conftil_acc <- inner_join(conftil_incorrect, conftil_correct, by=c("confidence", "response"))
conftil_acc <- ungroup(conftil_acc)
conftil_acc <- conftil_acc %>% add_row(confidence=7, response=0 ,prob_acc.x=0, prob.x=0, prob0=0, prob_acc.y=0, prob.y=0, prob1=0) %>% add_row(confidence=7, response=1 ,prob_acc.x=0, prob.x=0, prob0=0, prob_acc.y=0, prob.y=0, prob1=0)

conftil_acc$response[conftil_acc$response=="0"] <- "Vertical"
conftil_acc$response[conftil_acc$response=="1"] <- "Tilted"

#calculating area under the curve
auc_tilt <- t(sapply(split(conftil_acc, conftil_acc$response), function(x) {
  data.frame(response=x$response[1], auc=auc(x$prob0, x$prob1, type='spline'))
}))
AUC_til <- merge(conftil_acc, auc_tilt)
AUC_til$auc <- as.numeric(AUC_til$auc)
auc_til <- AUC_til$'auc'
names(auc_til) <-AUC_til$response
```
## 4.4.Metacognitive sensitivity
Metacognitive sensitivity, which is quantified as the area under Type 2 ROC curve (see    \ref{fig:type2 ROC} ), is significantly higher for Yes (`r apa(unname(auc_det['Yes']), 2, T) `) than No (`r apa(unname(auc_det['No']), 2, T)`) response. Similar pattern is observed in the tilt recognition task, where the AUC is significantly higher for Tilted (`r apa(unname(auc_til['Tilted']), 2, T)`) than Vertical (`r apa(unname(auc_til['Vertical']), 2, T)`) responses. This suggest that participants confidence ratings are more diagnostic to accuracy in the judgments of a target stimulus being pressent than being absent. Similarly, the correct judgments of a stimulus being tilted is better reflected by partcipants' confidence ratings than the correct judgments of a stimulus being vertical. 

```{r echo=FALSE, message=FALSE, fig.cap="\\label{fig:type2 ROC} Type 2 ROC curve for each task.", cache=TRUE, fig,with=3.3, fig.height=3.3}
#################################### 
# Plotting type 2 ROC
####################################
roc2_det <- ggplot(data=confdet_acc, aes(x=prob0, y=prob1)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 4) + 
  scale_color_manual(values=c("#e41a1c", "#377eb8")) +
  labs(title="Detection", x= "p(conf | correct)", y="p(conf | incorrect)", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0, 1),breaks=number_ticks(6)) + scale_y_continuous(limits = c(0, 1), breaks=number_ticks(6)) +
  theme(legend.position=c(0.8, 0.2),  
                 legend.background = element_blank(),
        legend.title = element_blank(),
                 legend.key = element_blank(), 
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
roc2_det


roc2_dis <- ggplot(data=confdis_acc, aes(x=prob0, y=prob1)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 4) + 
  scale_color_manual(values=c("#984ea3", "#4daf4a")) +
  labs(title="Discrimination", x= "p(conf | correct)", y="p(conf | incorrect)", color="Response")+ 
  theme_classic() + scale_x_continuous(limits = c(0, 1),breaks=number_ticks(6)) + scale_y_continuous(limits = c(0, 1), breaks=number_ticks(6)) +
  theme(legend.position=c(0.8, 0.2),  
                 legend.background = element_blank(),
        legend.title = element_blank(),
                 legend.key = element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic")))
roc2_dis

roc2_til <- ggplot(data=conftil_acc, aes(x=prob0, y=prob1)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 4) + 
  scale_color_manual(values=c("#999999","#f781bf")) +
  labs(title="Tilt Recognition", x= "p(conf | correct)", y="p(conf | incorrect)", color="Response")+ 
  theme_classic()+ scale_x_continuous(limits = c(0, 1),breaks=number_ticks(6)) + scale_y_continuous(limits = c(0, 1), breaks=number_ticks(6)) +
  theme(legend.position=c(0.8, 0.2),  
                 legend.title = element_blank(),
        legend.background = element_blank(),
                 legend.key = element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic")))
roc2_til
```

&nbsp;

&nbsp;

## 4.5.BOLD signal in ROIs
```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3.2, cache=TRUE}
####################################
# BOLD data preparation detection 
####################################
Data_Det <- Data_Det %>%
  filter(! BOLD_rTPJ=="NaN")
Data_Det
Data_Det$response[Data_Det$response=="0"] <- "No"
Data_Det$response[Data_Det$response=="1"] <- "Yes"

#get the mean activation at each ROI for each participants at each confidence level for each response
BOLD_det_subj <- aggregate(Data_Det[, 16:21], list(Data_Det$subj_id, Data_Det$confidence, Data_Det$response, Data_Det$task ), mean)

#MM: what is the meaning of these column names (Group.1 etc?)
BOLD_det_subj <- rename(BOLD_det_subj,c('subj_id'='Group.1', 'confidence'='Group.2', 'response'='Group.3', 'task'='Group.4'))

#get the group mean of activation for each ROI
BOLD_det_mean <- aggregate(BOLD_det_subj[, 5:10], list(BOLD_det_subj$confidence, BOLD_det_subj$response), mean)
#get the se of activation for each ROI
BOLD_det_se <- aggregate(BOLD_det_subj[, 5:10], list(BOLD_det_subj$confidence, BOLD_det_subj$response ), se)
#combine with mean df==> ".x==mean, .y==se"

#MM: It's generally not a good idea to use the same variable name twice for different things (see line 1060)
BOLD_det_mean <- inner_join(BOLD_det_mean, BOLD_det_se, by=c("Group.1", "Group.2"))
#rename the colum names 
BOLD_det_mean <- rename(BOLD_det_mean,c('confidence'='Group.1', 'response'='Group.2'))

######################################### 
# BOLD data preparation discrimination 
#########################################

# MM: same here, using a function is better :)
Data_Dis <- Data_Dis %>%
  filter(! BOLD_rTPJ=="NaN")
Data_Dis
Data_Dis$response[Data_Dis$response=="0"] <- "Anticlockwise"
Data_Dis$response[Data_Dis$response=="1"] <- "Clockwise"

#get the mean activation at each ROI for each participants at each confidence level for each response
BOLD_dis_subj <- aggregate(Data_Dis[, 16:21], list(Data_Dis$subj_id, Data_Dis$confidence, Data_Dis$response, Data_Dis$task), mean)
#rename the colum names 
BOLD_dis_subj <- rename(BOLD_dis_subj,c('subj_id'='Group.1', 'confidence'='Group.2', 'response'='Group.3', 'task'='Group.4'))
#get the group mean of activation for each ROI
BOLD_dis_mean <- aggregate(BOLD_dis_subj[, 5:10], list(BOLD_dis_subj$confidence, BOLD_dis_subj$response ), mean)
#get the se of activation for each ROI
BOLD_dis_se <- aggregate(BOLD_dis_subj[, 5:10], list(BOLD_dis_subj$confidence, BOLD_dis_subj$response ), se)
#combine with mean df==> ".x==mean, .y==se"
BOLD_dis_mean <- inner_join(BOLD_dis_mean, BOLD_dis_se, by=c("Group.1", "Group.2"))
#rename the colum names 
BOLD_dis_mean <- rename(BOLD_dis_mean,c('confidence'='Group.1', 'response'='Group.2'))

#################################### 
# BOLD data preparation tilt 
####################################
Data_Til <- Data_Til %>%
  filter(! BOLD_rTPJ=="NaN")
Data_Til
Data_Til$response[Data_Til$response=="0"] <- "Vertical"
Data_Til$response[Data_Til$response=="1"] <- "Tilted"

#get the mean activation at each ROI for each participants at each confidence level for each response
BOLD_til_subj <- aggregate(Data_Til[, 16:21], list(Data_Til$subj_id, Data_Til$confidence, Data_Til$response, Data_Til$task ), mean)
#rename the colum names 
BOLD_til_subj <- rename(BOLD_til_subj,c('subj_id'='Group.1', 'confidence'='Group.2', 'response'='Group.3', 'task'='Group.4'))
#get the group mean of activation for each ROI
BOLD_til_mean <- aggregate(BOLD_til_subj[, 5:10], list(BOLD_til_subj$confidence, BOLD_til_subj$response ), mean)
#get the se of activation for each ROI
BOLD_til_se <- aggregate(BOLD_til_subj[, 5:10], list(BOLD_til_subj$confidence, BOLD_til_subj$response ), se)
#combine with mean df==> ".x==mean, .y==se"
BOLD_til_mean <- inner_join(BOLD_til_mean, BOLD_til_se, by=c("Group.1", "Group.2"))
#rename the colum names 
BOLD_til_mean <- rename(BOLD_til_mean,c('confidence'='Group.1', 'response'='Group.2'))
```

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}
#################################### 
# Plotting rTPJ 
#################################### 
rTPJ_det <- ggplot(data=BOLD_det_mean, aes(x=confidence, y=BOLD_rTPJ.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_rTPJ.x-BOLD_rTPJ.y, ymax = BOLD_rTPJ.x+BOLD_rTPJ.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#e41a1c", "#377eb8")) +
  labs(title="rTPJ detection", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = element_blank(),
  legend.text = (element_text(size = 8)),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
rTPJ_det

rTPJ_dis <- ggplot(data=BOLD_dis_mean, aes(x=confidence, y=BOLD_rTPJ.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_rTPJ.x-BOLD_rTPJ.y, ymax = BOLD_rTPJ.x+BOLD_rTPJ.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#984ea3", "#4daf4a")) +
  labs(title="rTPJ discrimination", x= "confidence", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.85, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = element_blank(),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
rTPJ_dis

rTPJ_til <- ggplot(data=BOLD_til_mean, aes(x=confidence, y=BOLD_rTPJ.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_rTPJ.x-BOLD_rTPJ.y, ymax = BOLD_rTPJ.x+BOLD_rTPJ.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#999999","#f781bf")) +
  labs(title="rTPJ tilt recognition", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.7, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = element_blank(),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
rTPJ_til
```
&nbsp;

&nbsp;

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}
#################################### 
# Plotting vmPFC 
#################################### 
vmPFC_det <- ggplot(data=BOLD_det_mean, aes(x=confidence, y=BOLD_vmPFC.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_vmPFC.x-BOLD_vmPFC.y, ymax = BOLD_vmPFC.x+BOLD_vmPFC.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#e41a1c", "#377eb8")) +
  labs(title="vmPFC detection", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1.5, 0.5), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = element_blank(),
  legend.text = (element_text(size = 8)),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
vmPFC_det

vmPFC_dis <- ggplot(data=BOLD_dis_mean, aes(x=confidence, y=BOLD_vmPFC.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_vmPFC.x-BOLD_vmPFC.y, ymax = BOLD_vmPFC.x+BOLD_vmPFC.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#984ea3", "#4daf4a")) +
  labs(title="vmPFC detection", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1.5, 0.5), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = element_blank(),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
vmPFC_dis

vmPFC_til <- ggplot(data=BOLD_til_mean, aes(x=confidence, y=BOLD_vmPFC.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_vmPFC.x-BOLD_vmPFC.y, ymax = BOLD_vmPFC.x+BOLD_vmPFC.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#999999","#f781bf")) +
  labs(title="vmPFC detection", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1.5, 0.5), breaks=number_ticks(6))+
  theme(legend.position=c(0.7, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = element_blank(),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
vmPFC_til
```
&nbsp;

&nbsp;

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}
#################################### 
# Plotting pMFC
#################################### 
pMFC_det <- ggplot(data=BOLD_det_mean, aes(x=confidence, y=BOLD_pMFC.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_pMFC.x-BOLD_pMFC.y, ymax = BOLD_pMFC.x+BOLD_pMFC.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#e41a1c", "#377eb8")) +
  labs(title="pMFC detection ", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(0.2, 1.8), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
pMFC_det

pMFC_dis <- ggplot(data=BOLD_dis_mean, aes(x=confidence, y=BOLD_pMFC.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_pMFC.x-BOLD_pMFC.y, ymax = BOLD_pMFC.x+BOLD_pMFC.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#984ea3", "#4daf4a")) +
  labs(title="pMFC discrimination", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(0.2, 1.8), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
pMFC_dis

pMFC_til <- ggplot(data=BOLD_til_mean, aes(x=confidence, y=BOLD_pMFC.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_pMFC.x-BOLD_pMFC.y, ymax = BOLD_pMFC.x+BOLD_pMFC.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#999999","#f781bf")) +
  labs(title="pMFC tilt", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(0.2, 1.8), breaks=number_ticks(6))+
  theme(legend.position=c(0.7, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
pMFC_til
```
&nbsp;

&nbsp;

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}
#################################### 
# Plotting FPl
#################################### 
FPl_det <- ggplot(data=BOLD_det_mean, aes(x=confidence, y=BOLD_FPl.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_FPl.x-BOLD_FPl.y, ymax = BOLD_FPl.x+BOLD_FPl.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#e41a1c", "#377eb8")) +
  labs(title="FPl detection ", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-0.5, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
FPl_det

FPl_dis <- ggplot(data=BOLD_dis_mean, aes(x=confidence, y=BOLD_FPl.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_FPl.x-BOLD_FPl.y, ymax = BOLD_FPl.x+BOLD_FPl.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#984ea3", "#4daf4a")) +
  labs(title="FPl discrimination", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-0.5, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
FPl_dis

FPl_til <- ggplot(data=BOLD_til_mean, aes(x=confidence, y=BOLD_FPl.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_FPl.x-BOLD_FPl.y, ymax = BOLD_FPl.x+BOLD_FPl.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#999999","#f781bf")) +
  labs(title="FPl tilt", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-0.5, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.7, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
FPl_til
```
&nbsp;

<!-- MM: Are those nbsps necessary? -->

&nbsp;

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}
#################################### 
# Plotting FPm
#################################### 
FPm_det <- ggplot(data=BOLD_det_mean, aes(x=confidence, y=BOLD_FPm.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_FPm.x-BOLD_FPm.y, ymax = BOLD_FPm.x+BOLD_FPm.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#e41a1c", "#377eb8")) +
  labs(title="FPm detection ", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
FPm_det

FPm_dis <- ggplot(data=BOLD_dis_mean, aes(x=confidence, y=BOLD_FPm.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_FPm.x-BOLD_FPm.y, ymax = BOLD_FPm.x+BOLD_FPm.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#984ea3", "#4daf4a")) +
  labs(title="FPm discrimination", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.8, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
FPm_dis

FPm_til <- ggplot(data=BOLD_til_mean, aes(x=confidence, y=BOLD_FPm.x)) +
  geom_line(aes(colour = factor(response))) +
  geom_point(aes(color = factor(response)), size = 3) + 
  geom_errorbar( aes(ymin = BOLD_FPm.x-BOLD_FPm.y, ymax = BOLD_FPm.x+BOLD_FPm.y, colour = factor(response)), width = 0.1)+
  scale_color_manual(values=c("#999999","#f781bf")) +
  labs(title="FPm tilt", x= "confidence", y="mean activation", color="Response") +
  theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1, 1), breaks=number_ticks(6))+
  theme(legend.position=c(0.7, 0.15),  
                 legend.background = element_blank(),
                 legend.key = element_blank(),
        legend.title = (element_text(size = 8)),
  legend.text = (element_text(size = 8)),
  axis.title.y=element_blank(),
        plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
FPm_til
```

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}

BOLD_mean_subj <- rbind(BOLD_det_subj, BOLD_dis_subj, BOLD_til_subj)

#linear effect of conf (as continuous variables) on ROIs
lm_conf_vmPFC <- lm(formula = BOLD_vmPFC~confidence, data = BOLD_mean_subj)
lm_conf_pMFC <- lm(formula = BOLD_pMFC~confidence, data = BOLD_mean_subj)
lm_conf_rTPJ <- lm(formula = BOLD_rTPJ~confidence, data = BOLD_mean_subj)
lm_conf_FPl <- lm(formula = BOLD_FPl~confidence, data = BOLD_mean_subj)
lm_conf_FPm <- lm(formula = BOLD_FPm~confidence, data = BOLD_mean_subj)

#pull out beta and p values
b_lm_vmPFC <- coef(summary(lm_conf_vmPFC))[, "Estimate"]
b_lm_pMFC <- coef(summary(lm_conf_pMFC))[, "Estimate"]
b_lm_rTPJ <- coef(summary(lm_conf_rTPJ))[, "Estimate"]
b_lm_FPl <- coef(summary(lm_conf_FPl))[, "Estimate"]
b_lm_FPm <- coef(summary(lm_conf_FPm))[, "Estimate"]

p_lm_vmPFC <- coef(summary(lm_conf_vmPFC))[, "Pr(>|t|)"]
p_lm_pMFC <- coef(summary(lm_conf_pMFC))[, "Pr(>|t|)"]
p_lm_rTPJ <- coef(summary(lm_conf_rTPJ))[, "Pr(>|t|)"]
p_lm_FPl <- coef(summary(lm_conf_FPl))[, "Pr(>|t|)"]
p_lm_FPm <- coef(summary(lm_conf_FPm))[, "Pr(>|t|)"]

as.numeric(b_lm_vmPFC)
as.numeric(b_lm_pMFC)
as.numeric(b_lm_rTPJ)
as.numeric(b_lm_FPl)
as.numeric(b_lm_FPm)
as.numeric(p_lm_vmPFC)
as.numeric(p_lm_pMFC)
as.numeric(p_lm_rTPJ)
as.numeric(p_lm_FPl)
as.numeric(p_lm_FPm)

#lm with interactor task
lm_task_vmPFC <- lm(formula = BOLD_vmPFC~confidence+ confidence:task, data = BOLD_mean_subj)
lm_task_rTPJ <- lm(formula = BOLD_rTPJ~confidence+ confidence:task, data = BOLD_mean_subj)
lm_task_mPFC <- lm(formula = BOLD_pMFC~confidence+ confidence:task, data = BOLD_mean_subj)
lm_task_FPl <- lm(formula = BOLD_FPl~confidence+ confidence:task, data = BOLD_mean_subj)
lm_task_FPm <- lm(formula = BOLD_FPm~confidence+ confidence:task, data = BOLD_mean_subj)

#lm with interactor response detection
lm_resp_vmPFC <- lm(formula = BOLD_vmPFC~confidence+ confidence:response, data = BOLD_det_subj)
lm_resp_rTPJ <- lm(formula = BOLD_rTPJ~confidence+ confidence:response, data = BOLD_det_subj)
lm_resp_mPFC <- lm(formula = BOLD_pMFC~confidence+ confidence:response, data = BOLD_det_subj)
lm_resp_FPl <- lm(formula = BOLD_FPl~confidence+ confidence:response, data = BOLD_det_subj)
lm_resp_FPm <- lm(formula = BOLD_FPm~confidence+ confidence:response, data = BOLD_det_subj)

#lm with interactor response discrimination
lm_resp_vmPFC <- lm(formula = BOLD_vmPFC~confidence+ confidence:response, data = BOLD_dis_subj)
lm_resp_rTPJ <- lm(formula = BOLD_rTPJ~confidence+ confidence:response, data = BOLD_dis_subj)
lm_resp_mPFC <- lm(formula = BOLD_pMFC~confidence+ confidence:response, data = BOLD_dis_subj)
lm_resp_FPl <- lm(formula = BOLD_FPl~confidence+ confidence:response, data = BOLD_dis_subj)
lm_resp_FPm <- lm(formula = BOLD_FPm~confidence+ confidence:response, data = BOLD_dis_subj)

#lm with interactor response tilt
lm_resp_vmPFC <- lm(formula = BOLD_vmPFC~confidence+ confidence:response, data = BOLD_til_subj)
lm_resp_rTPJ <- lm(formula = BOLD_rTPJ~confidence+ confidence:response, data = BOLD_til_subj)
lm_resp_mPFC <- lm(formula = BOLD_pMFC~confidence+ confidence:response, data = BOLD_til_subj)
lm_resp_FPl <- lm(formula = BOLD_FPl~confidence+ confidence:response, data = BOLD_til_subj)
lm_resp_FPm <- lm(formula = BOLD_FPm~confidence+ confidence:response, data = BOLD_til_subj)

```
## Effect of confidence on BOLD signal in ROIs
From our data, negative linear confidence-related effects were observed in right Temporoparietal Junction (rTPJ) ($\beta$=`r apa(b_lm_rTPJ[2],2,T)`, p=`r apa(p_lm_rTPJ[2],3,T)`), Posterior Medial Frontal Cortex (pMFC)($\beta$=`r apa(b_lm_pMFC[2], 2,T)`, p=`r myround_p(apa(p_lm_pMFC[2], 3,T))`), as well as polsitive linear correlation between confidence and BOLD signals in Ventromedial Prefrontal Cortex (vmPFC)($\beta$=`r apa(b_lm_vmPFC[2],2,T)`, p=`r myround_p(apa(p_lm_vmPFC[2],3,T))`), Medial Frontopolar Cortex (FPm)($\beta$=`r apa(b_lm_FPm[2],2,T)`, p=`r apa(p_lm_FPm[2], 3,T)`). 

To investigate whether linear effects of confidence on BOLD signal were influenced by the type of task (Detection vs. Discrimination vs. Tilt recognition) and/or reponse (Yes vs. NO; Clockwise vs. Anticlockwise; Vertical vs. Tilted), linear models with interactions were tested. The effects of confidence failed to show a significant difference between three tasks in all ROIs (all p values > 0.23). No sognificant effect was observed between Yes and No response in detection (all p values > 0.11), between Clockwise and Anticlockwise responses in discrimination (all p values > 0.33) or between Vertical and Tilted responses in tilt recognition (all p values > 0.14).

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3}
#non-linear effect of conf and response (as categorical variables) on rTPJ
aov.resp_rTPJ_det <- aov(BOLD_rTPJ~response,data=BOLD_det_subj)
aov.conf_rTPJ_det <- aov(BOLD_rTPJ~confidence,data=BOLD_det_subj)

aov.resp_rTPJ_dis <- aov(BOLD_rTPJ~response,data=BOLD_dis_subj)
aov.conf_rTPJ_dis <- aov(BOLD_rTPJ~confidence,data=BOLD_dis_subj)

aov.resp_rTPJ_til <- aov(BOLD_rTPJ~response,data=BOLD_til_subj)
aov.conf_rTPJ_til <- aov(BOLD_rTPJ~confidence,data=BOLD_til_subj)

summary(aov.resp_rTPJ_det)
summary(aov.conf_rTPJ_det)
summary(aov.resp_rTPJ_dis)
summary(aov.conf_rTPJ_dis)
summary(aov.resp_rTPJ_til)
summary(aov.conf_rTPJ_til)

#effect of conf and response on vmPFC
```
&nbsp;


```{r  mixed_effects, echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=2.5, fig.height=3 , cache=TRUE}
color_scheme <- tibble(Detection=c("#e41a1c", "#377eb8"), 
                       Discrimination=c("#984ea3", "#4daf4a"),
                       Tilt=c("#999999","#f781bf"))

plot_re_coefficients <- function(cur_task, ROI, df) {
  
  df$conf_as_factor <- factor(df$confidence)

  # fit mixed effects model:
  coefs <- lmer(
    paste('BOLD_',ROI,' ~ conf_as_factor*response + (1|subj_id)',sep=''), #note that this is a random intercept only model!
    df%>%filter(task==cur_task)) %>%
    tidy()
  
  # add confidence and response rows
  coefs$confidence =  c(1:6,1:6, NA, NA) # the last two rows are for random effects
  coefs$response = c(rep(0,6), rep(1,6), NA, NA)
  
  # plot
  plot <-ggplot(data=coefs, aes(x=confidence, y=estimate)) +
    geom_line(aes(colour = factor(response))) +
    geom_point(aes(color = factor(response)), size = 3) + 
    geom_errorbar( aes(ymin = estimate+std.error, ymax =estimate-std.error, colour = factor(response)), width = 0.1)+
    scale_color_manual(values=color_scheme[[cur_task]]) +
    labs(title=paste(ROI,cur_task), x= "confidence", color="Response") +
    theme_classic() + scale_x_continuous(limits = c(0.5, 6),breaks=number_ticks(6)) + scale_y_continuous(limits = c(-1.5, 1.5), breaks=number_ticks(6))+
    theme(legend.position=c(0.7, 0.15),  
                   legend.background = element_blank(),
                   legend.key = element_blank(),
          legend.title = (element_text(size = 8)),
    legend.text = (element_text(size = 8)),
          plot.title = (element_text(color = "black", size = 12, face = "italic"))) 
  print(plot)
  
}

for (ROI in list('rTPJ','vmPFC','pMFC','FPl','FPm')) {
  for (task in list('Detection','Discrimination','Tilt')) {
    print(ROI)
    plot_re_coefficients(task,ROI,Data_Valid)
  }
}
```
&nbsp;

## Reference 
