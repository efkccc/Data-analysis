---
title: "Behavioural Analysis"
author: "Chudi Gong"
date: "22/05/2020"
output: pdf_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tinytex)
library(dplyr)
library(magrittr)
library(ggplot2)
library(readr)
library(tidyr)
library(RColorBrewer)
library(pROC)
getwd()
```

```{r include=FALSE}
#open file
Behavioural_data <- read.csv("Behaviouraldata.csv")

#creat a new file with only includedtrials 
Behavioural_Valid <- Behavioural_data %>% 
  #Filter out the excluded trials 
  filter(inclusion==1)
Behavioural_Valid 

#get number of subjects
Behavioural_subj <- Behavioural_data %>% 
  #Filter out the excluded trials 
  filter(inclusion==1) %>%
  group_by(subj_id) %>%
  summarise(totaltrials=(count=n()))
Behavioural_subj

Behavioural_Excluded <- Behavioural_Valid %>%
  filter(! response=="NaN") 
Behavioural_Excluded

Behavioural_Det <- Behavioural_Excluded %>% 
  #only include detection 
  filter(task=="Detection")
Behavioural_Det

Behavioural_Dis <- Behavioural_Excluded %>% 
  #only include detection 
  filter(task=="Discrimination")
Behavioural_Dis

Behavioural_Til<- Behavioural_Excluded %>% 
  #only include detection 
  filter(task=="Tilt")
Behavioural_Til

#count the total no of trials for each task for each participant
total_trials <- Behavioural_Valid %>%
  group_by(subj_id, task) %>%
  summarise(totaltrials=(count=n()))
total_trials

p_trials <- Behavioural_Valid %>%
  group_by(subj_id, task) %>%
  filter(response=="1") %>%
  summarise(p_trials=(count=n()))
p_trials

#count correct trial for each task
individual_trials <- Behavioural_Valid %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id, task, accuracy) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  #combine with the total data frame
  inner_join(total_trials, by = c ("subj_id","task")) %>%
  mutate(proportion = no_trials / totaltrials)
individual_trials

#combine with the ptrials data frame
trials <- Behavioural_Valid %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id, task, accuracy, response) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  inner_join(p_trials, by = c ("subj_id","task"))
trials

#display accuracy 
individual_accuracy <- individual_trials %>%
  filter(accuracy==1) %>%
  rename(correct=proportion)
individual_accuracy

#display accuracy for each task 
task_accuracy <- individual_accuracy %>%
  group_by(task) %>%
  summarise(mean(correct))
task_accuracy

#creat a new file with only included trials + valid confidence rating 
Behavioural_Excluded <- Behavioural_Valid %>% 
  #Filter out the excluded trials 
  filter(! response=="NaN") 
Behavioural_Excluded

#get number of subjects
no_subj <- nrow(Behavioural_subj)

#mean confidence frequency 
confidence_distribution <- Behavioural_Excluded %>%
  group_by(task,confidence,response) %>%
  summarise(no_trials=(count=n())) %>%
  mutate(frequency = no_trials / no_subj)
confidence_distribution

#creat a new file with only detection 
confidence_detection <- confidence_distribution %>% 
  #only include detection 
  filter(task=="Detection")
confidence_detection

confidence_detection$response[confidence_detection$response=="0"] <- "No"
confidence_detection$response[confidence_detection$response=="1"] <- "Yes"

#creat a new file with only discrimination 
confidence_discrimination <- confidence_distribution %>% 
  #only include discrimination 
  filter(task=="Discrimination") 
confidence_discrimination
confidence_discrimination$response[confidence_discrimination$response=="0"]<-"Anticlockwise"
confidence_discrimination$response[confidence_discrimination$response=="1"]<-"Clockwise"

#creat a new file with only tilt recognition 
confidence_tilt <- confidence_distribution %>% 
  #only include til recognition 
  filter(task=="Tilt") 
confidence_tilt

confidence_tilt$response[confidence_tilt$response=="0"] <-"Vertical"
confidence_tilt$response[confidence_tilt$response=="1"] <-"Tilted"

#calculating HIT and FA rate
hit_detect <- individual_trials %>% 
  filter(task=="Detection", accuracy=="1") %>%
  summarize(no_trials=(count = n()))
hit_detect

```

## 1.Performance across different tasks.

```{r, echo=FALSE, message=FALSE}
number_ticks <- function(n) {function(limits) pretty(limits, n)}
data <- individual_accuracy
ggplot(data, mapping = aes(x= task, y=correct)) + labs(title="Fig.1 Mean accuracy", y="mean accuracy")+ theme_classic() +
   geom_boxplot()+ geom_jitter(position=position_jitter(width=.1, height=0), alpha=0.3, size=4) + scale_y_continuous(breaks=number_ticks(10))

```


```{r, include=FALSE}
data1 <- task_accuracy
getmean <- data1$'mean(correct)'
names(getmean) <-data1$task

#ANOVA comparing task performance 
aov.Mean_accuracy <- aov(correct~task,data=individual_accuracy)
summary(aov.Mean_accuracy)
p1 <- summary(aov.Mean_accuracy)[[1]][["Pr(>F)"]][[1]]
F1 <- summary(aov.Mean_accuracy)[[1]][["F value"]][[1]]

#calculating d' rate for detection
hit_det <- trials %>% 
  filter(task=="Detection", response== "1", accuracy=="1") %>%
  mutate(hit_rate = no_trials / p_trials) %>%
  mutate(z_hit= qnorm(hit_rate))
hit_det
det_hit <- mean(hit_det$hit_rate)

fa_det <- trials %>% 
  filter(task=="Detection", response== "1", accuracy=="0") %>%
  mutate(fa_rate = no_trials / p_trials) %>%
  mutate(z_fa= qnorm(fa_rate)) %>%
  inner_join(hit_det, by = c ("subj_id","task", "response", "p_trials")) %>%
  mutate(dprime= z_hit-z_fa)
fa_det

det_fa <- mean(fa_det$fa_rate)

#calculating d' for discrimination
hit_dis <- trials %>% 
  filter(task=="Discrimination", response== "1", accuracy=="1") %>%
  mutate(hit_rate = no_trials / p_trials) %>%
  mutate(z_hit= qnorm(hit_rate))
hit_dis

fa_dis<- trials %>% 
  filter(task=="Discrimination", response== "1", accuracy=="0") %>%
  mutate(fa_rate = no_trials / p_trials)%>%
  mutate(z_fa= qnorm(fa_rate)) %>%
  inner_join(hit_dis, by = c ("subj_id","task", "response", "p_trials")) %>%
  mutate(dprime= z_hit-z_fa)
fa_dis

#calculating d' rate for tilt
hit_til <- trials %>% 
  filter(task=="Tilt", response== "1", accuracy=="1") %>%
  mutate(hit_rate = no_trials / p_trials) %>%
  mutate(z_hit= qnorm(hit_rate))
hit_til

til_hit <- mean(hit_til$hit_rate)

fa_til<- trials %>% 
  filter(task=="Tilt", response== "1", accuracy=="0") %>%
  mutate(fa_rate = no_trials / p_trials) %>%
  mutate(z_fa= qnorm(fa_rate)) %>%
  inner_join(hit_til, by = c ("subj_id","task", "response", "p_trials")) %>%
  mutate(dprime= z_hit-z_fa) 
fa_til
til_fa <- mean(fa_til$fa_rate)

#merge together 
d_prime <- rbind(fa_det, fa_dis, fa_til)

#d values for each task
d_det <- mean(fa_det$dprime)
d_dis <- mean(fa_dis$dprime)
d_til <- mean(fa_til$dprime)

#ANOVA Calculation for accuracy
aov.d <- aov(dprime~task, data=d_prime)
summary(aov.d)
p2 <- summary(aov.d)[[1]][["Pr(>F)"]][[1]]
F2<- summary(aov.d)[[1]][["F value"]][[1]]

#response bias 
response <- Behavioural_Excluded %>% 
  #group by ID, type of task and accuracy
  group_by(subj_id,task, response) %>% 
  #count number of correct/incorrect/NAH trials 
  summarize(no_trials=(count = n())) %>%
  #combine with the total data frame
  inner_join(total_trials, by = c ("task","subj_id")) %>%
  mutate(probability = no_trials / totaltrials)
response

det_resp <- response %>%
  filter(task=="Detection") %>%
  filter(response=="1")
det_resp

dis_resp <- response %>%
  filter(task=="Discrimination") %>%
  filter(response=="1")
dis_resp

til_resp <- response %>%
  filter(task=="Tilt") %>%
  filter(response=="1")
til_resp

det_bias <- mean(det_resp$probability)
sd_bias1 <- sd(det_resp$probability)

detection<- (rnorm(no_subj, mean = det_bias, sd = sd_bias1))
t.test(detection, mu = 0.5) # Ho: mu = 0.5

dis_bias <- mean(dis_resp$probability)
sd_bias2 <- sd(dis_resp$probability)
discrimination<- (rnorm(no_subj, mean = dis_bias, sd = sd_bias2))
t.test(discrimination, mu = 0.5) # Ho: mu = 0.5

til_bias <- mean(til_resp$probability)
sd_bias3 <- sd(til_resp$probability)
tilt<- (rnorm(no_subj, mean = til_bias, sd = sd_bias3))
t.test(tilt, mu = 0.5) # Ho: mu = 0.5

#calculating rt
rt <- Behavioural_Excluded %>%
  group_by(subj_id, task, response, accuracy) %>%
  summarise(mean(response_time)) %>%
  rename("mean_rt"="mean(response_time)")
rt

#calculating mean rt for correct and incorrect trials
rt_accuracy <- rt %>%
  group_by(accuracy) %>%
  summarise(mean(mean_rt)) %>%
  rename("mean_rt"="mean(mean_rt)") 
rt_accuracy

rt_accuracy$accuracy[rt_accuracy$accuracy=="0"]<-"incorrect"
rt_accuracy$accuracy[rt_accuracy$accuracy=="1"]<-"correct"

#calling out mean rt for correct and incorrect trials
getmean1 <- rt_accuracy$'mean_rt'
names(getmean1) <-rt_accuracy$accuracy

#calculating sd of rt for correct and incorrect trials
correct_trials <- rt %>%
  filter(accuracy==1)
correct_trials

incorrect_trials <- rt %>%
  filter(accuracy==0)
incorrect_trials 

sd_rt1 <- sd(correct_trials$mean_rt)
sd_rt0 <- sd(incorrect_trials$mean_rt)

p.det <- t.test(response_time~response, mu=0, alt="two.sided",conf=0.95,data=Behavioural_Det)$p.value
p.dis <-t.test(response_time~response, mu=0, alt="two.sided", conf=0.95,data=Behavioural_Dis)$p.value
ttil <-t.test(response_time~response, mu=0, alt="two.sided", conf=0.95,data=Behavioural_Til)$statistic
p.til <-t.test(response_time~response, data=Behavioural_Til)$p.value

vertical <- Behavioural_Til %>%
  filter(response=="0")
vertical

tilted <- Behavioural_Til %>%
  filter(response=="1")
tilted

```
### Mean performance for each task 
Performan across the three tasks, detection (accuracy= `r format(round(unname(getmean['Detection']), 2), nsmall = 2)`, d'= `r format(round(print(d_det), 2), nsmall = 2)`),  discrimination (accuracy= `r format(round(unname(getmean['Discrimination']), 2), nsmall = 2)`, d'= `r format(round(print(d_dis), 2), nsmall = 2)`) and tilt recognition (accuracy = `r format(round(unname(getmean['Tilt']), 2), nsmall = 2)`, d'= `r format(round(print(d_til), 2), nsmall = 2)`) was similar. An one-way ANOVA failed to detect a significant difference between the accuracy of these threee tasks (F= `r format(round(F1, 2), nsmall=2)`, p= `r format(round(p1, 2), nsmall=2)`, ) and d' (F= `r format(round(F2, 2), nsmall=2)`, p= `r format(round(p2, 2), nsmall=2)`).

The probability of responding YES in detection was `r format(round(print(det_bias), 2), nsmall = 2)` ± `r format(round(print(sd_bias1), 2), nsmall = 2)`), and was significantly different from 0.5. The probability of responding CLOCKWISE was `r format(round(print(dis_bias), 2), nsmall = 2)` ± `r format(round(print(sd_bias2), 2), nsmall = 2)`) and was not significantly different from 0.5. For the tilt recognition task, the probability of responding TILTED was`r format(round(print(til_bias), 2), nsmall = 2)` ± `r format(round(print(sd_bias3), 2), nsmall = 2)`).

Response time was faster on average for correct response (`r format(round(unname(getmean1['correct']), 2), nsmall = 2)` ± `r format(round(print(sd_rt1), 2), nsmall = 2)` seconds) than incorrect responses (`r format(round(unname(getmean1['incorrect']), 2), nsmall = 2)` ± `r format(round(print(sd_rt0), 2), nsmall = 2)`seconds). A one-way analyssi of variance failed to detect a significant overall effect of responses type in detection (YES vs. NO, p=`r format(round(print(p.det), 2), nsmall = 2)` ), discrimination (CLOCKWISE vs. ANTICLOCKWISE, p= `r format(round(print(p.dis), 2), nsmall = 2)`) and tilt recognition (t=`r format(round(print(ttil), 2), nsmall = 2)`), p= `r format(round(print(p.til), 2), nsmall = 2)`). 

## 2.Confidence distributions
```{r echo=FALSE}
number_ticks <- function(n) {function(limits) pretty(limits, n)}

#SE calculation  (the standard deviation divided by the square root of the sample size)
se <- function(x) sqrt(var(x)/length(x))
pd <- position_dodge(0.5) # move them .05 to the left and right
ci <- se(confidence_detection$frequency)

#plot the frequency of confidence ratings for detection
data2 <- confidence_detection
cbPalette <- c("#e41a1c", "#377eb8")
confidence_det <- ggplot(data2, mapping = aes(x = confidence, y= frequency, fill= response)) +
  geom_bar(stat="identity", width = 0.65, position = position_dodge(width = 0.7))+ scale_fill_manual(values=cbPalette) + theme_classic() + labs(title="Fig.2 Detection", y="frequency")+ scale_x_continuous(breaks=number_ticks(6)) + geom_errorbar(aes(ymin=frequency-ci, ymax=frequency+ci), width=.1, position=pd)
confidence_det

cii <- se(confidence_discrimination$frequency)
data3 <- confidence_discrimination
caPalette <- c("#4daf4a", "#984ea3")

#plot the frequency of confidence ratings for discrimination
confidence_dis <- ggplot(data3, mapping = aes(x = confidence, y= frequency, fill=response)) +
  geom_bar(stat="identity", width = 0.65, position = position_dodge(width = 0.7))+ scale_fill_manual(values=caPalette) + theme_classic() + labs(title="Fig.3 Discrimination", y="frequency")+ scale_x_continuous(breaks=number_ticks(6)) +
  geom_errorbar(aes(ymin=frequency-cii, ymax=frequency+cii), width=.1, position=pd)
confidence_dis

ciii <- se(confidence_tilt$frequency)
data4 <- confidence_tilt
ccPalette <- c("#fc8d62", "#8da0cb")
#plot the frequency of confidence ratings for tilt recognition 
confidence_til<- ggplot(data4, mapping = aes(x = confidence, y= frequency, fill= response)) +
  geom_bar(stat="identity", width = 0.65, position = position_dodge(width = 0.7))+ scale_fill_manual(values=ccPalette) + theme_classic() + labs(title="Fig.4 Tilt recognition", y="frequency")+ scale_x_continuous(breaks=number_ticks(6)) +
  geom_errorbar(aes(ymin=frequency-ciii, ymax=frequency+ciii), width=.1, position=pd)
confidence_til
```

```{r include=FALSE}
det <- Behavioural_Excluded %>%
  filter(task=="Detection") 
det

dis <- Behavioural_Excluded %>%
  filter(task=="Discrimination") 
dis

til <- Behavioural_Excluded %>%
  filter(task=="Tilt") 
til

p1 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = det)$p.value
t1 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = det)$statistic
p3 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = til)$p.value
t3 <- t.test(confidence~response, mu=0, alt="two.sided", conf=0.95, data = til)$statistic
```
Within detection, a significant difference in mean confidence was observed between YES (target present) and NO (target absent) responses (see Fig.4 above), such that participants are more confident in their YES responses (t=`r format(round(print(t1), 2), nsmall = 2)`, p = `r format(round(print(p1), 2), nsmall = 2)`) and a statistical significance was also observed in the tilt recognition task between TILTED and Vertical response (t=`r format(round(print(t3), 2), nsmall = 2)`, p = `r format(round(print(p3), 2), nsmall = 2)`).

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

``` {r include=FALSE }

#ROC preparation
sti <- Behavioural_Excluded %>%
  group_by(subj_id, task, stimulus) %>%
  summarise(total_sti_trials=(count=n())) 
sti

conf_prob <- Behavioural_Excluded %>%
  group_by(subj_id, task, stimulus, response, accuracy, confidence) %>%
  summarise(no_trials=(count=n())) %>%
  inner_join(sti, by = c ("subj_id","task", "stimulus")) %>%
  mutate(probability= no_trials/total_sti_trials) %>%
  mutate(prob= confidence*probability)
conf_prob

#ROC detection 
conf_prob_det1 <- conf_prob %>%
  filter(task=="Detection", response=="1")
conf_prob_det1

conf_prob_det0 <- conf_prob %>%
  filter(task=="Detection", response=="0")
conf_prob_det0

#ROC discrimination 
conf_prob_dis1 <- conf_prob %>%
  filter(task=="Discrimination",response=="1")
conf_prob_dis1 

conf_prob_dis0 <- conf_prob %>%
  filter(task=="Discrimination", response=="0")
conf_prob_dis0 

#ROC tilt
conf_prob_til1 <- conf_prob %>%
  filter(task=="Tilt", response=="1")
conf_prob_til1 

conf_prob_til0 <- conf_prob %>%
  filter(task=="Tilt", response=="0")
conf_prob_til0 

```

```{r echo=FALSE, results = FALSE, message=FALSE, warning=FALSE, fig.width=3.8, fig.height=3.8}
####################################
#ROC detection plotting
####################################
par(pty="s")
roc(conf_prob_det1$accuracy, conf_prob_det1$prob, plot=TRUE, legacy.axes=TRUE,
    percent = TRUE, xlab="False positive rate", ylab="Hit rate", main="Fig.5 Detection Type 1 ROC", col="#377eb8", print.auc=TRUE)
plot.roc (conf_prob_det0$accuracy, conf_prob_det0$prob,
          percent = TRUE, add=TRUE, col="#e41a1c", print.auc=TRUE, print.auc.y=38)
legend("bottomright", legend = c("Yes", "No"), col=c("#377eb8","#e41a1c"),cex=0.8, lwd=3)

####################################
#ROC discrimination plotting
####################################
roc(conf_prob_dis1$accuracy, conf_prob_dis1$prob, plot=TRUE, legacy.axes=TRUE,
    percent = TRUE, xlab="False positive rate", ylab="Hit rate", main="Fig.6 Discrimination Type 1 ROC", col="#984ea3", print.auc=TRUE)
roc.dis <- plot.roc (conf_prob_dis0$accuracy, conf_prob_dis0$prob,
                     percent = TRUE, add=TRUE, col="#4daf4a", print.auc=TRUE, print.auc.y=38)
legend("bottomright", legend = c("Clockwise", "Anticlockwise"), col=c("#984ea3","#4daf4a"), cex=0.8,lwd=3)

#####################################
#ROC tilt plotting
#####################################
roc(conf_prob_dis1$accuracy, conf_prob_dis1$prob, plot=TRUE, legacy.axes=TRUE,
    percent = TRUE, xlab="False positive rate", ylab="Hit rate", main="Fig.7 Tilt Type 1 ROC", col="#8da0cb", print.auc=TRUE)
plot.roc (conf_prob_dis0$accuracy, conf_prob_dis0$prob,
          percent = TRUE, add=TRUE, col="#fc8d62", print.auc=TRUE, print.auc.y=38)
legend("bottomright", legend = c("Tilted", "Vertical"), cex=0.8, col=c("#8da0cb","#fc8d62"), lwd=3)

```
&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

Citations [@denison2018pnasusa][@mazor2020e]
&nbsp;

## Reference 
